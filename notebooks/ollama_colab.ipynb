{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollama-header"
      },
      "source": [
        "# Backdoor AI - Ollama on Google Colab (Memory Optimized)\n",
        "\n",
        "This notebook helps you run Ollama on Google Colab to use with your Backdoor AI application. You can install and run Ollama models (including Llama4) directly in Colab, then connect your Backdoor AI app to it.\n",
        "\n",
        "## How it works\n",
        "\n",
        "1. This notebook will first optimize your Colab environment for large models\n",
        "2. We'll install Ollama with special modifications for Colab\n",
        "3. You'll select and download models (Llama4 or others)\n",
        "4. We'll set up Cloudflared to create a secure tunnel to your Ollama instance\n",
        "5. You'll get a URL to use in your Backdoor AI settings\n",
        "\n",
        "This version is specially optimized for memory efficiency with large models.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memory-optimization"
      },
      "source": [
        "## 1. Memory Optimization for Large Models\n",
        "\n",
        "First, let's clear up disk space and optimize memory to ensure we have enough resources for large models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-optimization-code"
      },
      "outputs": [],
      "source": [
        "# Memory optimization functions\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import gc\n",
        "import time\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Install required packages first\n",
        "!pip install -q psutil\n",
        "import psutil\n",
        "\n",
        "def clear_disk_space():\n",
        "    \"\"\"Clean up disk space by removing unnecessary files.\"\"\"\n",
        "    print(\"🧹 Cleaning up disk space...\")\n",
        "    \n",
        "    # Clean apt cache\n",
        "    subprocess.run(\"apt-get clean\", shell=True)\n",
        "    \n",
        "    # Remove unnecessary packages\n",
        "    subprocess.run(\"apt-get -y autoremove\", shell=True)\n",
        "    \n",
        "    # Clean pip cache\n",
        "    subprocess.run(\"rm -rf ~/.cache/pip\", shell=True)\n",
        "    \n",
        "    # Remove temporary files\n",
        "    temp_dirs = ['/tmp', '/var/tmp']\n",
        "    for temp_dir in temp_dirs:\n",
        "        if os.path.exists(temp_dir):\n",
        "            try:\n",
        "                for item in os.listdir(temp_dir):\n",
        "                    item_path = os.path.join(temp_dir, item)\n",
        "                    # Skip our ollama directories\n",
        "                    if item.startswith('ollama') or item.startswith('backdoor'):\n",
        "                        continue\n",
        "                    \n",
        "                    try:\n",
        "                        if os.path.isdir(item_path):\n",
        "                            shutil.rmtree(item_path)\n",
        "                        else:\n",
        "                            os.remove(item_path)\n",
        "                    except Exception as e:\n",
        "                        pass  # Skip files that can't be removed\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not clean {temp_dir}: {e}\")\n",
        "    \n",
        "    # Remove unused Docker images/containers if Docker is installed\n",
        "    try:\n",
        "        subprocess.run(\"docker system prune -af\", shell=True, stderr=subprocess.DEVNULL)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"✅ Disk cleanup complete!\")\n",
        "    show_disk_usage()\n",
        "\n",
        "def show_disk_usage():\n",
        "    \"\"\"Show current disk usage.\"\"\"\n",
        "    try:\n",
        "        df_output = subprocess.check_output(\"df -h /\", shell=True, text=True)\n",
        "        print(\"\\n📊 Disk Space Available:\")\n",
        "        for line in df_output.split('\\n'):\n",
        "            print(line)\n",
        "    except:\n",
        "        print(\"Could not retrieve disk usage information\")\n",
        "\n",
        "def show_memory_usage():\n",
        "    \"\"\"Show current memory usage.\"\"\"\n",
        "    try:\n",
        "        memory = psutil.virtual_memory()\n",
        "        total_gb = memory.total / (1024 ** 3)\n",
        "        available_gb = memory.available / (1024 ** 3)\n",
        "        used_gb = memory.used / (1024 ** 3)\n",
        "        percent = memory.percent\n",
        "        \n",
        "        print(f\"\\n📊 Memory Usage:\")\n",
        "        print(f\"Total Memory: {total_gb:.2f} GB\")\n",
        "        print(f\"Available: {available_gb:.2f} GB\")\n",
        "        print(f\"Used: {used_gb:.2f} GB ({percent}%)\")\n",
        "    except:\n",
        "        print(\"Could not retrieve memory usage information\")\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear Python memory.\"\"\"\n",
        "    gc.collect()\n",
        "    torch_available = False\n",
        "    \n",
        "    try:\n",
        "        import torch\n",
        "        torch_available = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    if torch_available:\n",
        "        try:\n",
        "            import torch\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"✅ PyTorch CUDA cache cleared\")\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    print(\"✅ Python memory cleared\")\n",
        "\n",
        "def clean_model_files(keep_models=None):\n",
        "    \"\"\"Clean up model files to free space, optionally keeping specified models.\"\"\"\n",
        "    if keep_models is None:\n",
        "        keep_models = []\n",
        "    \n",
        "    print(f\"🧹 Cleaning model files (keeping: {', '.join(keep_models) if keep_models else 'none'})...\")\n",
        "    \n",
        "    # Clean Ollama model files (except the ones specified to keep)\n",
        "    ollama_dirs = ['/root/.ollama', '/tmp/ollama']\n",
        "    \n",
        "    for ollama_dir in ollama_dirs:\n",
        "        if os.path.exists(ollama_dir):\n",
        "            models_path = os.path.join(ollama_dir, 'models')\n",
        "            if os.path.exists(models_path):\n",
        "                for model_file in os.listdir(models_path):\n",
        "                    should_keep = False\n",
        "                    for keep_model in keep_models:\n",
        "                        if keep_model in model_file:\n",
        "                            should_keep = True\n",
        "                            break\n",
        "                    \n",
        "                    if not should_keep:\n",
        "                        try:\n",
        "                            model_path = os.path.join(models_path, model_file)\n",
        "                            if os.path.isdir(model_path):\n",
        "                                shutil.rmtree(model_path)\n",
        "                            else:\n",
        "                                os.remove(model_path)\n",
        "                            print(f\"  - Removed: {model_file}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"  - Could not remove {model_file}: {e}\")\n",
        "    \n",
        "    print(\"✅ Model cleanup complete!\")\n",
        "\n",
        "def monitor_download_progress(model_name):\n",
        "    \"\"\"Monitor the download progress of a model.\"\"\"\n",
        "    last_size = 0\n",
        "    download_dir = '/root/.ollama/models'\n",
        "    \n",
        "    print(f\"🔄 Monitoring download progress for {model_name}\")\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            if not os.path.exists(download_dir):\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "                \n",
        "            total_size = 0\n",
        "            for root, dirs, files in os.walk(download_dir):\n",
        "                for file in files:\n",
        "                    if model_name.lower() in file.lower():\n",
        "                        try:\n",
        "                            file_path = os.path.join(root, file)\n",
        "                            total_size += os.path.getsize(file_path)\n",
        "                        except:\n",
        "                            pass\n",
        "            \n",
        "            if total_size > last_size:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Downloading {model_name}...\")\n",
        "                print(f\"Downloaded: {total_size / (1024**3):.2f} GB\")\n",
        "                last_size = total_size\n",
        "            \n",
        "            time.sleep(2)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Download monitoring stopped\")\n",
        "\n",
        "# Run optimization process\n",
        "print(\"🚀 Optimizing environment for large language models...\")\n",
        "clear_disk_space()\n",
        "clear_memory()\n",
        "\n",
        "# Set environment variables for improved performance\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "\n",
        "# Show current resource usage\n",
        "show_memory_usage()\n",
        "show_disk_usage()\n",
        "\n",
        "print(\"\\n✅ Optimization complete! Ready to continue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu-check"
      },
      "source": [
        "## 2. Check GPU availability\n",
        "\n",
        "Let's check if a GPU is available for this Colab session. This will help with model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-gpu"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "import subprocess\n",
        "import gc\n",
        "import os\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Check if GPU is available\n",
        "gpu_available = torch.cuda.is_available()\n",
        "gpu_info = None\n",
        "\n",
        "if gpu_available:\n",
        "    # Clear CUDA cache if torch is available\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    gpu_info = {\n",
        "        \"name\": torch.cuda.get_device_name(0),\n",
        "        \"count\": torch.cuda.device_count(),\n",
        "        \"capability\": torch.cuda.get_device_capability(0),\n",
        "        \"memory\": torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB\n",
        "    }\n",
        "    display(HTML(f'''\n",
        "    <div style=\"background-color:#d4edda; padding:10px; border-radius:5px; margin:10px 0;\">\n",
        "        <h3 style=\"color:#155724;\">✅ GPU Detected</h3>\n",
        "        <p><b>GPU Name:</b> {gpu_info['name']}</p>\n",
        "        <p><b>GPU Memory:</b> {gpu_info['memory']:.2f} GB</p>\n",
        "        <p><b>CUDA Capability:</b> {gpu_info['capability'][0]}.{gpu_info['capability'][1]}</p>\n",
        "        <p>Your Colab instance has a GPU available. This will significantly improve model performance.</p>\n",
        "    </div>'''))\n",
        "else:\n",
        "    display(HTML('''\n",
        "    <div style=\"background-color:#f8d7da; padding:10px; border-radius:5px; margin:10px 0;\">\n",
        "        <h3 style=\"color:#721c24;\">⚠️ No GPU Detected</h3>\n",
        "        <p>Your Colab instance doesn't have a GPU. Models will run slower.</p>\n",
        "        <p>Consider changing the runtime type to GPU:</p>\n",
        "        <p><b>Runtime > Change runtime type > Hardware accelerator > GPU</b></p>\n",
        "    </div>'''))\n",
        "    \n",
        "# Show memory and disk usage\n",
        "show_memory_usage()\n",
        "show_disk_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "requirements-section"
      },
      "source": [
        "## 3. Set up environment\n",
        "\n",
        "Now let's install Ollama and required packages. We'll use a modified installation script to prevent systemd-related warnings and optimize for memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-requirements"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Clean up unnecessary packages to save space\n",
        "apt-get clean\n",
        "apt-get -y autoremove\n",
        "\n",
        "# Install necessary packages first\n",
        "apt-get update && apt-get install -y curl wget lshw pciutils\n",
        "\n",
        "# Clean up after installation\n",
        "apt-get clean\n",
        "\n",
        "# Create Ollama directories\n",
        "mkdir -p /tmp/ollama/models\n",
        "\n",
        "# Get the Ollama installer but don't run it directly\n",
        "curl -fsSL https://ollama.com/install.sh -o /tmp/ollama_install.sh\n",
        "\n",
        "# Modify the install script to handle Colab environment (no systemd)\n",
        "sed -i 's/systemctl daemon-reload/echo \"Skipping systemctl: Not using systemd in Colab environment\"/g' /tmp/ollama_install.sh\n",
        "sed -i 's/systemctl enable ollama/echo \"Skipping systemd service setup in Colab environment\"/g' /tmp/ollama_install.sh\n",
        "sed -i 's/systemctl start ollama/echo \"Starting Ollama manually instead of via systemd\"/g' /tmp/ollama_install.sh\n",
        "\n",
        "# Run the modified installer\n",
        "chmod +x /tmp/ollama_install.sh\n",
        "OLLAMA_YES=1 /tmp/ollama_install.sh\n",
        "\n",
        "# Install cloudflared for tunneling\n",
        "wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "dpkg -i cloudflared-linux-amd64.deb\n",
        "rm cloudflared-linux-amd64.deb  # Remove the deb file after installation\n",
        "\n",
        "# Install minimal Python dependencies\n",
        "pip install -q requests httpx ipywidgets \n",
        "\n",
        "# Clean pip cache\n",
        "rm -rf ~/.cache/pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start-ollama"
      },
      "source": [
        "## 4. Start Ollama server\n",
        "\n",
        "Now we'll start the Ollama server with GPU acceleration if available and proper memory management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-ollama-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "# Clear Python memory before starting server\n",
        "gc.collect()\n",
        "if gpu_available:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Set environment variables for Ollama\n",
        "os.environ[\"OLLAMA_ORIGINS\"] = \"*\"\n",
        "os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
        "\n",
        "# If GPU available, set optimized CUDA params\n",
        "if gpu_available:\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    # Set low-level GPU memory optimizations\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "# Kill any existing Ollama processes\n",
        "!pkill -f ollama || true\n",
        "time.sleep(1)\n",
        "\n",
        "# Start Ollama server in background\n",
        "print(\"Starting Ollama server...\")\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True,\n",
        "    env=os.environ\n",
        ")\n",
        "\n",
        "# Wait for Ollama to start\n",
        "max_attempts = 3\n",
        "attempt = 0\n",
        "server_started = False\n",
        "\n",
        "while attempt < max_attempts and not server_started:\n",
        "    attempt += 1\n",
        "    print(f\"Starting attempt {attempt}/{max_attempts}...\")\n",
        "    time.sleep(5)  # Give it time to start\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/version\")\n",
        "        if response.status_code == 200:\n",
        "            server_started = True\n",
        "            version_info = response.json()\n",
        "            # Get memory usage after Ollama starts\n",
        "            memory = psutil.virtual_memory()\n",
        "            available_gb = memory.available / (1024 ** 3)\n",
        "            used_gb = memory.used / (1024 ** 3)\n",
        "            \n",
        "            display(HTML(f'''\n",
        "            <div style=\"background-color:#d4edda; padding:10px; border-radius:5px; margin:10px 0;\">\n",
        "                <h3 style=\"color:#155724;\">✅ Ollama started successfully!</h3>\n",
        "                <p><b>Version:</b> {version_info.get('version')}</p>\n",
        "                <p><b>GPU Acceleration:</b> {'Enabled' if gpu_available else 'Not available'}</p>\n",
        "                <p><b>Available Memory:</b> {available_gb:.2f} GB</p>\n",
        "            </div>'''))\n",
        "        else:\n",
        "            print(f\"❌ Ollama returned unexpected status: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to connect on attempt {attempt}: {e}\")\n",
        "        # Kill the process and try again if we still have attempts left\n",
        "        if attempt < max_attempts:\n",
        "            print(\"Restarting Ollama server...\")\n",
        "            if ollama_process:\n",
        "                ollama_process.terminate()\n",
        "                time.sleep(2)\n",
        "            # Clear memory before retrying\n",
        "            gc.collect()\n",
        "            if gpu_available:\n",
        "                torch.cuda.empty_cache()\n",
        "            ollama_process = subprocess.Popen(\n",
        "                [\"ollama\", \"serve\"],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True,\n",
        "                env=os.environ\n",
        "            )\n",
        "\n",
        "if not server_started:\n",
        "    display(HTML('''\n",
        "    <div style=\"background-color:#f8d7da; padding:10px; border-radius:5px; margin:10px 0;\">\n",
        "        <h3 style=\"color:#721c24;\">❌ Failed to start Ollama</h3>\n",
        "        <p>Could not start the Ollama server after multiple attempts.</p>\n",
        "        <p>Please check the output above for errors or try restarting the notebook.</p>\n",
        "    </div>'''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memory-prep"
      },
      "source": [
        "## 5. Prepare for model download\n",
        "\n",
        "Before downloading models, let's clear any temporary files and optimize further to ensure maximum available space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prep-download"
      },
      "outputs": [],
      "source": [
        "# Clear existing models (if any) to save space\n",
        "clean_model_files()\n",
        "\n",
        "# Run one more disk cleanup\n",
        "clear_disk_space()\n",
        "\n",
        "# Show available resources\n",
        "show_memory_usage()\n",
        "show_disk_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-section"
      },
      "source": [
        "## 6. Choose and download a model\n",
        "\n",
        "Now, let's download a model. We recommend models sized appropriately for your available resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-selection"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import threading\n",
        "\n",
        "# Check available memory to make better recommendations\n",
        "memory = psutil.virtual_memory()\n",
        "available_gb = memory.available / (1024 ** 3)\n",
        "disk_info = psutil.disk_usage('/')\n",
        "available_disk_gb = disk_info.free / (1024 ** 3)\n",
        "\n",
        "# Define recommended models based on GPU and available resources\n",
        "if gpu_available:\n",
        "    if available_gb > 30 and available_disk_gb > 80:\n",
        "        # High resources available\n",
        "        recommended_models = [\n",
        "            {\"name\": \"Llama4 (8B - Recommended)\", \"id\": \"llama4-8b:latest\", \"size\": \"~8GB\"},\n",
        "            {\"name\": \"Llama4 (70B - Requires lots of RAM)\", \"id\": \"llama4:latest\", \"size\": \"~70GB\"},\n",
        "            {\"name\": \"Llama4 Code (Code-specialized)\", \"id\": \"llama4-code:latest\", \"size\": \"~70GB\"},\n",
        "            {\"name\": \"Mistral (7B)\", \"id\": \"mistral:latest\", \"size\": \"~7GB\"},\n",
        "            {\"name\": \"Gemma (7B)\", \"id\": \"gemma:latest\", \"size\": \"~7GB\"},\n",
        "            {\"name\": \"Neural Chat (7B)\", \"id\": \"neural-chat:latest\", \"size\": \"~7GB\"},\n",
        "            {\"name\": \"Custom model (enter below)\", \"id\": \"custom\", \"size\": \"varies\"}\n",
        "        ]\n",
        "    elif available_gb > 15 and available_disk_gb > 20:\n",
        "        # Medium resources\n",
        "        recommended_models = [\n",
        "            {\"name\": \"Llama4 (8B - Recommended)\", \"id\": \"llama4-8b:latest\", \"size\": \"~8GB\"},\n",
        "            {\"name\": \"Mistral (7B)\", \"id\": \"mistral:latest\", \"size\": \"~7GB\"},\n",
        "            {\"name\": \"Gemma (7B)\", \"id\": \"gemma:latest\", \"size\": \"~7GB\"},\n",
        "            {\"name\": \"Neural Chat (7B)\", \"id\": \"neural-chat:latest\", \"size\": \"~7GB\"},\n",
        "            {\"name\": \"Custom model (enter below)\", \"id\": \"custom\", \"size\": \"varies\"}\n",
        "        ]\n",
        "    else:\n",
        "        # Low resources\n",
        "        recommended_models = [\n",
        "            {\"name\": \"Llama4 Tiny (Smallest - Recommended)\", \"id\": \"llama4-tiny:latest\", \"size\": \"~1.5GB\"},\n",
        "            {\"name\": \"Gemma (2B)\", \"id\": \"gemma:2b\", \"size\": \"~2GB\"},\n",
        "            {\"name\": \"Neural Chat (3B)\", \"id\": \"neural-chat:3b\", \"size\": \"~3GB\"},\n",
        "            {\"name\": \"Custom model (enter below)\", \"id\": \"custom\", \"size\": \"varies\"}\n",
        "        ]\n",
        "else:\n",
        "    # Without GPU, recommend smaller models regardless of RAM\n",
        "    recommended_models = [\n",
        "        {\"name\": \"Llama4 Tiny (Smallest - Recommended)\", \"id\": \"llama4-tiny:latest\", \"size\": \"~1.5GB\"},\n",
        "        {\"name\": \"Gemma (2B)\", \"id\": \"gemma:2b\", \"size\": \"~2GB\"},\n",
        "        {\"name\": \"Mistral Instruct (Small)\", \"id\": \"mistral-instruct:latest\", \"size\": \"~4GB\"},\n",
        "        {\"name\": \"Custom model (enter below)\", \"id\": \"custom\", \"size\": \"varies\"}\n",
        "    ]\n",
        "\n",
        "# Display available resources\n",
        "display(HTML(f'''\n",
        "<div style=\"background-color:#e9f5fb; padding:10px; border-radius:5px; margin:10px 0;\">\n",
        "    <h4 style=\"color:#0c63e4;\">Available Resources</h4>\n",
        "    <ul>\n",
        "        <li><b>Memory:</b> {available_gb:.2f} GB available</li>\n",
        "        <li><b>Disk:</b> {available_disk_gb:.2f} GB available</li>\n",
        "        <li><b>GPU:</b> {'Yes' if gpu_available else 'No'}</li>\n",
        "    </ul>\n",
        "    <p>Models are recommended based on your available resources.</p>\n",
        "</div>\n",
        "'''))\n",
        "\n",
        "# Create dropdown for model selection\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[(f\"{model['name']} ({model['size']})\", model['id']) for model in recommended_models],\n",
        "    description='Select model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "# Create text field for custom model\n",
        "custom_model = widgets.Text(\n",
        "    description='Custom model:',\n",
        "    placeholder='Enter model name (e.g., llama4-7b:latest)',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%', display='none')\n",
        ")\n",
        "\n",
        "# Download button\n",
        "download_button = widgets.Button(description='Download Model', button_style='primary')\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Function to handle model dropdown change\n",
        "def on_model_change(change):\n",
        "    if change['new'] == 'custom':\n",
        "        custom_model.layout.display = 'block'\n",
        "    else:\n",
        "        custom_model.layout.display = 'none'\n",
        "\n",
        "# Function to download model\n",
        "def download_model(b):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        model_id = model_dropdown.value\n",
        "        \n",
        "        if model_id == 'custom':\n",
        "            if not custom_model.value.strip():\n",
        "                print(\"⚠️ Please enter a custom model name!\")\n",
        "                return\n",
        "            model_id = custom_model.value.strip()\n",
        "        \n",
        "        # Run optimization before download\n",
        "        print(\"🧹 Clearing space for model download...\")\n",
        "        clear_disk_space()\n",
        "        clear_memory()\n",
        "        \n",
        "        print(f\"🚀 Downloading model: {model_id}\")\n",
        "        print(\"This may take a while depending on the model size and your internet connection...\")\n",
        "        print(\"You'll see progress below. Please don't interrupt the process.\")\n",
        "        \n",
        "        # Start a progress monitoring thread\n",
        "        monitor_thread = threading.Thread(target=monitor_download_progress, args=(model_id,))\n",
        "        monitor_thread.daemon = True\n",
        "        monitor_thread.start()\n",
        "        \n",
        "        # Run ollama pull command\n",
        "        process = subprocess.Popen(\n",
        "            [\"ollama\", \"pull\", model_id],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "        \n",
        "        # Show live output\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                print(output.strip())\n",
        "        \n",
        "        return_code = process.poll()\n",
        "        if return_code == 0:\n",
        "            print(f\"✅ Model {model_id} downloaded successfully!\")\n",
        "            # List available models\n",
        "            print(\"\\n📋 Available models:\")\n",
        "            !ollama list\n",
        "            \n",
        "            # Display memory usage after download\n",
        "            show_memory_usage()\n",
        "            show_disk_usage()\n",
        "        else:\n",
        "            print(f\"❌ Failed to download model {model_id}. Return code: {return_code}\")\n",
        "            print(\"\\nPossible reasons for failure:\")\n",
        "            print(\"- Not enough disk space\")\n",
        "            print(\"- Not enough memory\")\n",
        "            print(\"- Network issues\")\n",
        "            print(\"\\nTry selecting a smaller model or freeing up more space.\")\n",
        "\n",
        "# Connect events\n",
        "model_dropdown.observe(on_model_change, names='value')\n",
        "download_button.on_click(download_model)\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.HTML(f\"<h3>Select a model to download (with{'' if gpu_available else 'out'} GPU acceleration):</h3>\"))\n",
        "display(model_dropdown)\n",
        "display(custom_model)\n",
        "display(download_button)\n",
        "display(output_area)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-model"
      },
      "source": [
        "## 7. Test the model\n",
        "\n",
        "Let's make sure the model works by asking it a simple question. This also helps verify memory settings are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-model-code"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Clear memory before testing\n",
        "gc.collect()\n",
        "if gpu_available:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Function to test a model\n",
        "def test_model(model_id, prompt=\"Hi, I'm testing if you're working properly. Please give a brief greeting.\"):\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"response\": result.get(\"message\", {}).get(\"content\", \"No content returned\")\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": f\"Server returned status code {response.status_code}: {response.text}\"\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Get available models\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "    if response.status_code == 200:\n",
        "        models = response.json().get(\"models\", [])\n",
        "        model_options = [(model.get(\"name\"), model.get(\"name\")) for model in models]\n",
        "    else:\n",
        "        model_options = [(\"No models found\", \"\")]\n",
        "except Exception as e:\n",
        "    model_options = [(f\"Error: {str(e)}\", \"\")]\n",
        "\n",
        "# Create widgets\n",
        "test_model_dropdown = widgets.Dropdown(\n",
        "    options=model_options,\n",
        "    description='Model to test:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "test_prompt = widgets.Textarea(\n",
        "    value=\"Hi, I'm testing if you're working properly. Please give a brief greeting.\",\n",
        "    description='Prompt:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='80%', height='100px')\n",
        ")\n",
        "\n",
        "test_button = widgets.Button(description='Test Model', button_style='success')\n",
        "test_output = widgets.Output()\n",
        "\n",
        "# Function to test model\n",
        "def on_test_button_click(b):\n",
        "    with test_output:\n",
        "        clear_output()\n",
        "        if not test_model_dropdown.value:\n",
        "            print(\"⚠️ Please select a model to test!\")\n",
        "            return\n",
        "        \n",
        "        print(f\"🔍 Testing model '{test_model_dropdown.value}' with prompt: \\n{test_prompt.value}\\n\")\n",
        "        print(\"Waiting for response...\")\n",
        "        \n",
        "        # Clear memory before test\n",
        "        gc.collect()\n",
        "        if gpu_available:\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        start_time = time.time()\n",
        "        result = test_model(test_model_dropdown.value, test_prompt.value)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        \n",
        "        if result[\"success\"]:\n",
        "            print(f\"\\n✅ Model responded successfully in {elapsed_time:.2f} seconds!\\n\")\n",
        "            print(\"Response:\")\n",
        "            print(\"-----------------------------------\")\n",
        "            print(result[\"response\"])\n",
        "            print(\"-----------------------------------\")\n",
        "            \n",
        "            # Show memory usage after test\n",
        "            memory = psutil.virtual_memory()\n",
        "            available_gb = memory.available / (1024 ** 3)\n",
        "            used_gb = memory.used / (1024 ** 3)\n",
        "            print(f\"\\nMemory after response: {used_gb:.2f} GB used, {available_gb:.2f} GB available\")\n",
        "        else:\n",
        "            print(f\"\\n❌ Error testing model: {result['error']}\")\n",
        "            print(\"\\nTry to free up more memory by:\")\n",
        "            print(\"1. Running 'clear_memory()' in a new cell\")\n",
        "            print(\"2. Restart the runtime if needed\")\n",
        "            print(\"3. Try with a smaller model\")\n",
        "\n",
        "# Connect events\n",
        "test_button.on_click(on_test_button_click)\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.HTML(\"<h3>Test your model:</h3>\"))\n",
        "display(test_model_dropdown)\n",
        "display(test_prompt)\n",
        "display(test_button)\n",
        "display(test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-tunnel"
      },
      "source": [
        "## 8. Set up a tunnel to access your Ollama instance\n",
        "\n",
        "Now we'll set up a Cloudflare tunnel so your Backdoor AI application can access this Ollama instance. Copy the URL you get from this step into the Backdoor AI settings under the Ollama provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunnel-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Clean up memory before starting tunnel\n",
        "gc.collect()\n",
        "if gpu_available:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Kill any existing cloudflared processes\n",
        "!pkill -f cloudflared || true\n",
        "time.sleep(1)\n",
        "\n",
        "# Function to run cloudflared tunnel in a separate thread\n",
        "def run_tunnel():\n",
        "    global tunnel_process, tunnel_url\n",
        "    tunnel_process = subprocess.Popen(\n",
        "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "    \n",
        "    # Extract tunnel URL\n",
        "    tunnel_url = None\n",
        "    url_pattern = re.compile(r'https://[\\w.-]+\\.trycloudflare\\.com')\n",
        "    \n",
        "    while True:\n",
        "        line = tunnel_process.stdout.readline()\n",
        "        if not line and tunnel_process.poll() is not None:\n",
        "            break\n",
        "        \n",
        "        match = url_pattern.search(line)\n",
        "        if match and not tunnel_url:\n",
        "            tunnel_url = match.group(0)\n",
        "            print(f\"\\nTunnel URL found: {tunnel_url}\\n\")\n",
        "        \n",
        "        # Print all output for debugging\n",
        "        print(line.strip())\n",
        "\n",
        "# Start tunnel in a separate thread\n",
        "tunnel_url = None\n",
        "tunnel_thread = threading.Thread(target=run_tunnel)\n",
        "tunnel_thread.daemon = True\n",
        "tunnel_thread.start()\n",
        "\n",
        "# Wait for tunnel URL to be available\n",
        "attempts = 0\n",
        "max_attempts = 30\n",
        "while tunnel_url is None and attempts < max_attempts:\n",
        "    time.sleep(1)\n",
        "    attempts += 1\n",
        "    if attempts % 5 == 0:\n",
        "        print(f\"Waiting for tunnel URL... ({attempts}/{max_attempts} seconds)\")\n",
        "\n",
        "# Display connection information\n",
        "if tunnel_url:\n",
        "    display(HTML(f'''\n",
        "    <div style=\"background-color:#d4edda; padding:15px; border-radius:5px; margin:15px 0;\">\n",
        "        <h3 style=\"color:#155724;\">🔗 Tunnel Created Successfully</h3>\n",
        "        <p>Your Ollama instance is now accessible at:</p>\n",
        "        <div style=\"background-color:#f8f9fa; padding:10px; border-radius:5px; font-family:monospace; margin:10px 0;\">\n",
        "            <b>{tunnel_url}</b>\n",
        "        </div>\n",
        "        <h4 style=\"color:#155724; margin-top:15px;\">How to connect from Backdoor AI:</h4>\n",
        "        <ol>\n",
        "            <li>Go to Settings > LLM Provider Settings</li>\n",
        "            <li>Select \"Ollama\" as your provider</li>\n",
        "            <li>In the \"API Base URL\" field, enter: <b>{tunnel_url}</b></li>\n",
        "            <li>Select your model from the dropdown</li>\n",
        "            <li>Click \"Save Settings\"</li>\n",
        "        </ol>\n",
        "        <p><b>Important:</b> Keep this notebook running as long as you need the Ollama service. Closing it will terminate the tunnel.</p>\n",
        "    </div>'''))\n",
        "    \n",
        "    # Also provide a simple example cURL command to test\n",
        "    print(\"\\nTest your tunnel connection with this command (from any machine):\")\n",
        "    print(f\"curl -s {tunnel_url}/api/version\")\n",
        "else:\n",
        "    display(HTML('''\n",
        "    <div style=\"background-color:#f8d7da; padding:10px; border-radius:5px; margin:10px 0;\">\n",
        "        <h3 style=\"color:#721c24;\">❌ Failed to create tunnel</h3>\n",
        "        <p>Could not establish a cloudflared tunnel after multiple attempts.</p>\n",
        "        <p>Please check the output above for errors or try the tunnel setup again.</p>\n",
        "    </div>'''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## Memory Management Utilities\n",
        "\n",
        "If you encounter memory issues, you can run these commands to free up resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-utilities"
      },
      "outputs": [],
      "source": [
        "# This cell contains utility functions you can run if you encounter memory issues\n",
        "\n",
        "# Show current memory and disk usage\n",
        "show_memory_usage()\n",
        "show_disk_usage()\n",
        "\n",
        "# To clear Python memory, run:\n",
        "# clear_memory()\n",
        "\n",
        "# To free up disk space, run:\n",
        "# clear_disk_space()\n",
        "\n",
        "# To clean up model files (except the one you're using), run:\n",
        "# For example, to keep only llama4-tiny:\n",
        "# clean_model_files(['llama4-tiny'])\n",
        "\n",
        "# If you need to restart the Ollama server, run:\n",
        "# !pkill -f ollama\n",
        "# time.sleep(2)\n",
        "# !ollama serve &\n",
        "\n",
        "# If you need to restart the tunnel, run:\n",
        "# !pkill -f cloudflared\n",
        "# Then go back to the tunnel creation cell and run it again"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
