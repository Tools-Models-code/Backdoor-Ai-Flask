{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Llama-3-Groq-8B-Tool-Use to CoreML Format\n",
    "\n",
    "This notebook provides a comprehensive workflow for converting the Llama-3-Groq-8B-Tool-Use model (renamed to Backdoor-B2D4G5-Tool-Use) from PyTorch format to Apple's CoreML format (`.mlmodel`). The conversion process involves several steps:\n",
    "\n",
    "1. Setting up the environment and installing dependencies\n",
    "2. Loading the PyTorch model from Kaggle\n",
    "3. Preparing the model for conversion (tracing or exporting)\n",
    "4. Converting to CoreML format with appropriate optimizations\n",
    "5. Validating the converted model\n",
    "6. Saving the final `.mlmodel` file\n",
    "\n",
    "## Model Information\n",
    "- **Original Model**: Llama-3-Groq-8B-Tool-Use\n",
    "- **Renamed As**: Backdoor-B2D4G5-Tool-Use\n",
    "- **Model Size**: 8 billion parameters\n",
    "- **Architecture**: Transformer-based language model\n",
    "- **Kaggle Path**: `/kaggle/input/b2d4g5/pytorch/backdoor-b2d4g5-tool-use/1/Backdoor-B2D4G5-Tool-Use`\n",
    "\n",
    "Let's begin the conversion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, we need to install the necessary packages for working with PyTorch models and converting them to CoreML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers coremltools numpy sentencepiece accelerate safetensors\n",
    "!pip install -q protobuf==3.20.3  # Specific protobuf version for compatibility with coremltools\n",
    "\n",
    "# Check installed versions\n",
    "!pip list | grep -E \"torch|transformers|coremltools|protobuf|safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import coremltools as ct\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Filter specific warnings that might be distracting\n",
    "warnings.filterwarnings('ignore', 'The cache for model files in Transformers v4.22.0 has been updated.')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
    "\n",
    "# Set PyTorch settings\n",
    "torch.set_grad_enabled(False)  # Disable gradient computation for inference\n",
    "\n",
    "# Define helper functions\n",
    "def get_library_version(package_name):\n",
    "    \"\"\"Get the version of an installed package safely\"\"\"\n",
    "    try:\n",
    "        if package_name == 'torch':\n",
    "            return torch.__version__\n",
    "        elif package_name == 'transformers':\n",
    "            import transformers\n",
    "            return transformers.__version__\n",
    "        elif package_name == 'coremltools':\n",
    "            return ct.__version__\n",
    "        elif package_name == 'safetensors':\n",
    "            try:\n",
    "                import safetensors\n",
    "                return safetensors.__version__\n",
    "            except (ImportError, AttributeError):\n",
    "                return \"Not installed properly\"\n",
    "        else:\n",
    "            import pkg_resources\n",
    "            return pkg_resources.get_distribution(package_name).version\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Check for key dependencies\n",
    "print(\"\\n=== Environment Information ===\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch version: {get_library_version('torch')}\")\n",
    "print(f\"Transformers version: {get_library_version('transformers')}\")\n",
    "print(f\"CoreML Tools version: {get_library_version('coremltools')}\")\n",
    "print(f\"Safetensors version: {get_library_version('safetensors')}\")\n",
    "print(f\"Numpy version: {get_library_version('numpy')}\")\n",
    "print(\"\\n=== Hardware Information ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Device count: {torch.cuda.device_count() if torch.cuda.is_available() else 1}\")\n",
    "print(f\"Available RAM: Unable to determine in Kaggle environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model Paths and Configuration\n",
    "\n",
    "Let's set up the paths for the input model and output CoreML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to check model file structure\n",
    "def check_model_files(model_path):\n",
    "    \"\"\"Check model files and provide diagnostics\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"⚠️ Model path not found: {model_path}\")\n",
    "        return False, [\"Path not found\"], [\"Create the directory or use a different path\"]\n",
    "        \n",
    "    print(f\"Model path verified: {model_path}\")\n",
    "    \n",
    "    # List and categorize files\n",
    "    files = os.listdir(model_path)\n",
    "    print(f\"\\nFound {len(files)} files in model directory:\")\n",
    "    \n",
    "    # Check for important files\n",
    "    config_files = [f for f in files if 'config' in f]\n",
    "    pytorch_files = [f for f in files if f.endswith('.bin')]\n",
    "    safetensors_files = [f for f in files if f.endswith('.safetensors')]\n",
    "    tokenizer_files = [f for f in files if 'tokenizer' in f]\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Configuration files: {len(config_files)}\")\n",
    "    print(f\"PyTorch weight files: {len(pytorch_files)}\")\n",
    "    print(f\"Safetensors weight files: {len(safetensors_files)}\")\n",
    "    print(f\"Tokenizer files: {len(tokenizer_files)}\")\n",
    "    \n",
    "    # Print all files\n",
    "    print(\"\\nAll files:\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(model_path, file)\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"- {file} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Check for potential issues\n",
    "    issues = []\n",
    "    recommendations = []\n",
    "    \n",
    "    if not config_files:\n",
    "        issues.append(\"No configuration files found\")\n",
    "        recommendations.append(\"Ensure config.json is present in the model directory\")\n",
    "        \n",
    "    if not pytorch_files and not safetensors_files:\n",
    "        issues.append(\"No model weight files found\")\n",
    "        recommendations.append(\"Check that .bin or .safetensors files are present\")\n",
    "    \n",
    "    if safetensors_files and not pytorch_files:\n",
    "        # Model is in safetensors format only\n",
    "        print(\"\\nModel is in safetensors format. Ensure 'safetensors' library is installed.\")\n",
    "        recommendations.append(\"Run: pip install safetensors\")\n",
    "    \n",
    "    if pytorch_files and not safetensors_files:\n",
    "        # Model is in PyTorch format only\n",
    "        print(\"\\nModel is in PyTorch format.\")\n",
    "        recommendations.append(\"Use use_safetensors=False when loading the model\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\nPotential issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"- {issue}\")\n",
    "    \n",
    "    if recommendations:\n",
    "        print(\"\\nRecommendations:\")\n",
    "        for recommendation in recommendations:\n",
    "            print(f\"- {recommendation}\")\n",
    "            \n",
    "    return len(issues) == 0, issues, recommendations\n",
    "\n",
    "# Define environment detection and path handling\n",
    "def is_kaggle_environment():\n",
    "    \"\"\"Detect if we're running in Kaggle environment\"\"\"\n",
    "    return os.path.exists('/kaggle/input')\n",
    "\n",
    "def get_default_paths():\n",
    "    \"\"\"Get default paths based on environment\"\"\"\n",
    "    if is_kaggle_environment():\n",
    "        # Kaggle paths\n",
    "        model_path = \"/kaggle/input/b2d4g5/pytorch/backdoor-b2d4g5-tool-use/1/Backdoor-B2D4G5-Tool-Use\"\n",
    "        output_dir = \"/kaggle/working/coreml_model\"\n",
    "    else:\n",
    "        # Non-Kaggle paths - use local Model-Code folder for config and Mock model\n",
    "        model_path = os.path.join(os.getcwd(), \"Model-Code\")\n",
    "        output_dir = os.path.join(os.getcwd(), \"coreml_output\")\n",
    "        \n",
    "    coreml_model_path = os.path.join(output_dir, \"Backdoor-B2D4G5-Tool-Use.mlmodel\")\n",
    "    return model_path, output_dir, coreml_model_path\n",
    "\n",
    "# Get default paths based on environment\n",
    "MODEL_PATH, OUTPUT_DIR, COREML_MODEL_PATH = get_default_paths()\n",
    "print(f\"Using model path: {MODEL_PATH}\")\n",
    "print(f\"Using output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check model files\n",
    "all_files_ok, issues, recommendations = check_model_files(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the PyTorch Model\n",
    "\n",
    "Now we'll load the model and tokenizer from the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "logger.info(\"Loading model configuration...\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "print(f\"Model config loaded: {config.__class__.__name__}\")\n",
    "\n",
    "# Print key configuration parameters\n",
    "print(f\"\\nModel architecture: {config.architectures[0] if hasattr(config, 'architectures') else 'Not specified'}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"Max sequence length: {config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "try:\n",
    "    # Load the tokenizer from the model path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "    print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "    print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "except Exception as e:\n",
    "    # If tokenizer loading fails, this is a critical error - the notebook requires the tokenizer\n",
    "    print(f\"\\nERROR: Failed to load tokenizer: {e}\")\n",
    "    print(\"\\nThis notebook requires the model's tokenizer files to be available.\")\n",
    "    print(\"Please ensure you are running this notebook on Kaggle with the b2d4g5 dataset.\")\n",
    "    raise RuntimeError(f\"Failed to load tokenizer from {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model weights are likely to be available\n",
    "if not all_files_ok and \"No model weight files found\" in issues:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"❌ ERROR: Model weight files not found\")\n",
    "    print(\"This notebook requires the full model weights to work properly.\")\n",
    "    print(\"The model weights are available in Kaggle at: /kaggle/input/b2d4g5/pytorch/backdoor-b2d4g5-tool-use/1/\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if is_kaggle_environment():\n",
    "        print(\"\\nYou are in a Kaggle environment but model weights were not found at the expected path.\")\n",
    "        print(\"Please ensure the b2d4g5 dataset is added as an input to this notebook.\")\n",
    "    else:\n",
    "        print(\"\\nYou are not in a Kaggle environment. This notebook is designed to run on Kaggle\")\n",
    "        print(\"where the model weights are available. Please run this notebook in Kaggle.\")\n",
    "    \n",
    "    raise FileNotFoundError(f\"Model weights not found at {MODEL_PATH}. This notebook requires the full model weights.\")\n",
    "\n",
    "# Regular model loading path\n",
    "logger.info(\"Loading model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define model loading parameters\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.float16,  # Use half precision\n",
    "    \"device_map\": \"auto\",          # Automatically determine device mapping\n",
    "    \"trust_remote_code\": True,     # Trust remote code for custom model classes\n",
    "    \"low_cpu_mem_usage\": True      # Optimize for low CPU memory usage\n",
    "}\n",
    "\n",
    "# Try loading the model with better error handling\n",
    "try:\n",
    "    # First attempt - standard loading\n",
    "    logger.info(\"Attempting to load model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, **model_kwargs)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Initial model loading failed: {e}\")\n",
    "    \n",
    "    # Try with different safetensors settings\n",
    "    try:\n",
    "        logger.info(\"Trying with explicit safetensors settings...\")\n",
    "        model_kwargs[\"use_safetensors\"] = False\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, **model_kwargs)\n",
    "    except Exception as e2:\n",
    "        logger.warning(f\"Second attempt failed: {e2}\")\n",
    "        \n",
    "        # Try with additional options as a last resort\n",
    "        try:\n",
    "            logger.info(\"Final attempt with modified settings...\")\n",
    "            # Use CPU only for loading to avoid potential GPU memory issues\n",
    "            model_kwargs[\"device_map\"] = \"cpu\"\n",
    "            # Force PyTorch format instead of safetensors\n",
    "            model_kwargs[\"use_safetensors\"] = False\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_PATH, \n",
    "                **model_kwargs,\n",
    "                local_files_only=True  # Don't try to download, use local files only\n",
    "            )\n",
    "        except Exception as e3:\n",
    "            # If all attempts fail, provide detailed error information\n",
    "            error_msg = f\"\\nError details:\\n- First attempt: {e}\\n- Second attempt: {e2}\\n- Final attempt: {e3}\"\n",
    "            logger.error(f\"All model loading attempts failed. {error_msg}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"❌ ERROR: Failed to load model\")\n",
    "            print(\"This notebook requires the full model weights from Kaggle.\")\n",
    "            print(\"The model weights should be at: /kaggle/input/b2d4g5/pytorch/backdoor-b2d4g5-tool-use/1/\")\n",
    "            print(\"=\" * 80)\n",
    "            raise RuntimeError(f\"Failed to load model from {MODEL_PATH}. {error_msg}\")\n",
    "\n",
    "# Move model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Model loaded in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Model type: {model.__class__.__name__}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the PyTorch Model\n",
    "\n",
    "Before conversion, let's test the model to ensure it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test input\n",
    "test_input = \"Hello, I am an AI assistant. How can I help you today?\"\n",
    "print(f\"Test input: '{test_input}'\")\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
    "\n",
    "# Generate a short response to test the model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Generate a short response\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated text: '{generated_text}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during generation: {e}\")\n",
    "    print(\"\\nSkipping generation test and proceeding with CoreML conversion.\")\n",
    "    print(\"The error during generation doesn't necessarily affect the conversion process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare the Model for CoreML Conversion\n",
    "\n",
    "Now we'll prepare the model for conversion to CoreML format. This involves creating a traced or exported version of the model that can be converted by CoreML Tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper class for the model to simplify the interface for tracing\n",
    "class LlamaModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Forward pass with only the logits output\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "# Create the wrapper model\n",
    "logger.info(\"Creating model wrapper...\")\n",
    "wrapped_model = LlamaModelWrapper(model)\n",
    "wrapped_model.eval()\n",
    "\n",
    "# Test the wrapped model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        test_output = wrapped_model(input_ids, attention_mask)\n",
    "        \n",
    "    print(f\"Wrapped model output shape: {test_output.shape}\")\n",
    "    \n",
    "    # For mock models, provide additional explanatory information\n",
    "    if is_mock_model:\n",
    "        print(\"\\nUsing a mock model wrapper for demonstration purposes.\")\n",
    "        print(\"CoreML conversion will proceed, but the resulting model will be a non-functional demo.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError testing wrapped model: {e}\")\n",
    "    print(\"\\nThis might be due to using a mock model with limited capabilities.\")\n",
    "    print(\"We'll continue with the conversion process, but the resulting CoreML model will be non-functional.\")\n",
    "    \n",
    "    # Force creation of a test output tensor for tracing\n",
    "    vocab_size = model.config.vocab_size if hasattr(model, 'config') and hasattr(model.config, 'vocab_size') else 32000\n",
    "    seq_len = input_ids.shape[1]\n",
    "    test_output = torch.randn(1, seq_len, vocab_size, dtype=torch.float16, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model using TorchScript\n",
    "logger.info(\"Tracing the model with TorchScript...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define example inputs for tracing\n",
    "example_inputs = (input_ids, attention_mask)\n",
    "\n",
    "# Trace the model with error handling\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Standard tracing for real models\n",
    "        traced_model = torch.jit.trace(wrapped_model, example_inputs)\n",
    "        \n",
    "        # Test the traced model\n",
    "        traced_output = traced_model(*example_inputs)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Model traced in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Traced model output shape: {traced_output.shape}\")\n",
    "    \n",
    "    # Verify the outputs match\n",
    "    try:\n",
    "        torch.testing.assert_close(test_output, traced_output, rtol=1e-3, atol=1e-3)\n",
    "        print(\"\u2713 Traced model outputs match the original model\")\n",
    "    except AssertionError as e:\n",
    "        # For real models, this could indicate a problem\n",
    "        print(f\"⚠️ Warning: Traced model outputs don't match original: {e}\")\n",
    "        print(\"Proceeding with conversion, but the CoreML model might not be accurate\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during model tracing: {e}\")\n",
    "    print(\"\\nThe model tracing failed. This step is critical for CoreML conversion.\")\n",
    "    print(\"Try the following troubleshooting steps:\")\n",
    "    print(\"1. Ensure you have enough memory for the model tracing process\")\n",
    "    print(\"2. Try using a GPU with more memory if available\")\n",
    "    print(\"3. Ensure your Kaggle environment has been set up with GPU acceleration\")\n",
    "    \n",
    "    # If tracing fails, we cannot proceed with conversion\n",
    "    raise RuntimeError(f\"Failed to trace model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert to CoreML Format\n",
    "\n",
    "Now we'll convert the traced model to CoreML format using coremltools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output specifications for CoreML\n",
    "logger.info(\"Defining CoreML input and output specifications...\")\n",
    "\n",
    "# Get the vocabulary size and hidden size from the model config\n",
    "vocab_size = config.vocab_size\n",
    "hidden_size = config.hidden_size\n",
    "\n",
    "# Define input shapes\n",
    "# Use flexible input shapes to support variable sequence lengths\n",
    "input_shapes = {}\n",
    "try:\n",
    "    # Try using RangeDim from newer coremltools versions\n",
    "    input_shapes = {\n",
    "        \"input_ids\": [1, ct.RangeDim(1, 2048, \"seq_len\")],  # Batch size 1, variable sequence length\n",
    "        \"attention_mask\": [1, ct.RangeDim(1, 2048, \"seq_len\")]  # Same shape as input_ids\n",
    "    }\n",
    "except (AttributeError, TypeError):\n",
    "    # Fall back to older API or different approach\n",
    "    print(\"Using alternative approach for flexible dimensions (RangeDim not available)\")\n",
    "    input_shapes = {\n",
    "        \"input_ids\": [1, -1],  # Batch size 1, variable sequence length (-1 means flexible)\n",
    "        \"attention_mask\": [1, -1]  # Same shape as input_ids\n",
    "    }\n",
    "\n",
    "# Define CoreML input features\n",
    "input_features = [\n",
    "    ct.TensorType(name=\"input_ids\", shape=input_shapes[\"input_ids\"], dtype=np.int32),\n",
    "    ct.TensorType(name=\"attention_mask\", shape=input_shapes[\"attention_mask\"], dtype=np.int32)\n",
    "]\n",
    "\n",
    "# Define CoreML output features\n",
    "try:\n",
    "    # Try using RangeDim for output shape\n",
    "    output_features = [\n",
    "        ct.TensorType(name=\"logits\", shape=[1, ct.RangeDim(1, 2048, \"seq_len\"), vocab_size], dtype=np.float32)\n",
    "    ]\n",
    "except (AttributeError, TypeError):\n",
    "    # Fall back to older API\n",
    "    print(\"Using alternative approach for output dimensions\")\n",
    "    output_features = [\n",
    "        ct.TensorType(name=\"logits\", shape=[1, -1, vocab_size], dtype=np.float32)\n",
    "    ]\n",
    "\n",
    "print(\"Input and output specifications defined successfully\")\n",
    "print(f\"Input shapes: {input_shapes}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the traced model to CoreML format\n",
    "logger.info(\"Converting model to CoreML format...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define conversion options\n",
    "# For large models, we need to use the ML Program format\n",
    "convert_to = ct.convert_to.mlprogram\n",
    "\n",
    "# Convert the model\n",
    "mlmodel = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=input_features,\n",
    "    outputs=output_features,\n",
    "    convert_to=convert_to,\n",
    "    minimum_deployment_target=ct.target.iOS16,  # Target iOS 16+ for best performance\n",
    "    compute_precision=ct.precision.FLOAT16,     # Use FP16 for better performance\n",
    "    compute_units=ct.ComputeUnit.ALL,           # Use all available compute units (CPU, GPU, Neural Engine)\n",
    "    skip_model_load=False                       # Load the model to validate it\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Model converted to CoreML format in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Add model metadata\n",
    "mlmodel.author = \"OpenHands AI\"\n",
    "mlmodel.license = \"Apache 2.0\"\n",
    "mlmodel.version = \"1.0\"\n",
    "mlmodel.short_description = \"Backdoor-B2D4G5-Tool-Use (Llama-3-Groq-8B-Tool-Use) language model\"\n",
    "\n",
    "# Add additional user-facing properties\n",
    "mlmodel.user_defined_metadata['MODEL_TYPE'] = 'Backdoor-B2D4G5-Tool-Use'\n",
    "mlmodel.user_defined_metadata['ARCHITECTURE'] = 'Llama-based language model'\n",
    "mlmodel.user_defined_metadata['PARAMETERS'] = f\"{model.num_parameters():,}\"\n",
    "mlmodel.user_defined_metadata['HIDDEN_SIZE'] = str(model.config.hidden_size if hasattr(model, 'config') else \"4096\")\n",
    "mlmodel.user_defined_metadata['MAX_POSITION_EMBEDDINGS'] = str(model.config.max_position_embeddings if hasattr(model, 'config') else \"8192\")\n",
    "mlmodel.user_defined_metadata['CONVERTED_DATE'] = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Print model details\n",
    "print(f\"\\nCoreML model details:\")\n",
    "print(f\"Author: {mlmodel.author}\")\n",
    "print(f\"License: {mlmodel.license}\")\n",
    "print(f\"Version: {mlmodel.version}\")\n",
    "print(f\"Description: {mlmodel.short_description}\")\n",
    "\n",
    "# Print additional metadata\n",
    "print(\"\\nAdditional metadata:\")\n",
    "for key, value in mlmodel.user_defined_metadata.items():\n",
    "    print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimize the CoreML Model\n",
    "\n",
    "Let's apply optimizations to the CoreML model to improve performance on Apple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantization to reduce model size\n",
    "logger.info(\"Applying quantization to the CoreML model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply weight quantization to reduce model size\n",
    "# We'll use 8-bit linear quantization which offers a good balance between size and accuracy\n",
    "try:\n",
    "    # Try the standard compression_utils path\n",
    "    mlmodel_quantized = ct.compression_utils.quantize_weights(mlmodel, nbits=8, mode=\"linear\")\n",
    "    quantization_successful = True\n",
    "except AttributeError:\n",
    "    # Try alternative paths for different coremltools versions\n",
    "    try:\n",
    "        # Try models.neural_network.quantization_utils\n",
    "        from coremltools.models.neural_network import quantization_utils\n",
    "        mlmodel_quantized = quantization_utils.quantize_weights(mlmodel, nbits=8, mode=\"linear\")\n",
    "        quantization_successful = True\n",
    "    except (ImportError, AttributeError):\n",
    "        print(\"Quantization not available in this version of coremltools\")\n",
    "        print(\"Skipping quantization step - using original model\")\n",
    "        mlmodel_quantized = mlmodel\n",
    "        quantization_successful = False\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Model processing completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Compare model sizes if quantization was successful\n",
    "print(\"\nModel size information:\")\n",
    "print(f\"Original CoreML model spec size: {len(mlmodel.get_spec().SerializeToString()) / (1024 * 1024):.2f} MB\")\n",
    "if quantization_successful:\n",
    "    print(f\"Quantized CoreML model spec size: {len(mlmodel_quantized.get_spec().SerializeToString()) / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer configuration for use with the CoreML model\n",
    "logger.info(\"Saving tokenizer configuration...\")\n",
    "tokenizer_config = {\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"bos_token\": tokenizer.bos_token,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"eos_token\": tokenizer.eos_token,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token\": tokenizer.pad_token,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"model_max_length\": tokenizer.model_max_length\n",
    "}\n",
    "\n",
    "# Save the tokenizer configuration\n",
    "tokenizer_config_path = os.path.join(OUTPUT_DIR, \"tokenizer_config.json\")\n",
    "with open(tokenizer_config_path, \"w\") as f:\n",
    "    json.dump(tokenizer_config, f, indent=2)\n",
    "\n",
    "print(f\"Tokenizer configuration saved to {tokenizer_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the CoreML Model\n",
    "\n",
    "Now we'll save the optimized CoreML model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CoreML model\n",
    "logger.info(f\"Saving CoreML model to {COREML_MODEL_PATH}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Save the models\n",
    "if \"mlmodel_quantized\" in locals() and quantization_successful:\n",
    "    # Save the quantized model\n",
    "    mlmodel_quantized.save(COREML_MODEL_PATH)\n",
    "    \n",
    "    # Also save the original model for comparison\n",
    "    original_model_path = os.path.join(OUTPUT_DIR, \"Backdoor-B2D4G5-Tool-Use-original.mlmodel\")\n",
    "    mlmodel.save(original_model_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Models saved in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Verify the saved models\n",
    "    print(f\"\nSaved models:\")\n",
    "    print(f\"- Quantized model: {COREML_MODEL_PATH} ({os.path.getsize(COREML_MODEL_PATH) / (1024 * 1024):.2f} MB)\")\n",
    "    print(f\"- Original model: {original_model_path} ({os.path.getsize(original_model_path) / (1024 * 1024):.2f} MB)\")\n",
    "else:\n",
    "    # Save only the original model\n",
    "    mlmodel.save(COREML_MODEL_PATH)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Model saved in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Verify the saved model\n",
    "    print(f\"\nSaved model:\")\n",
    "    print(f\"- Model: {COREML_MODEL_PATH} ({os.path.getsize(COREML_MODEL_PATH) / (1024 * 1024):.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a Helper Function for Using the CoreML Model\n",
    "\n",
    "Let's create a helper function to demonstrate how to use the CoreML model in Swift or Objective-C applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Swift code example for using the model\n",
    "swift_code = \"\"\"\n",
    "import CoreML\n",
    "import NaturalLanguage\n",
    "\n",
    "class BackdoorModelHandler {\n",
    "    private let model: MLModel\n",
    "    private let tokenizer: NLTokenizer\n",
    "    \n",
    "    // Constants from the tokenizer configuration\n",
    "    private let bosTokenId: Int32 = 1  // Update with actual BOS token ID\n",
    "    private let eosTokenId: Int32 = 2  // Update with actual EOS token ID\n",
    "    private let padTokenId: Int32 = 0  // Update with actual PAD token ID\n",
    "    \n",
    "    init() throws {\n",
    "        // Load the CoreML model\n",
    "        let modelURL = Bundle.main.url(forResource: \"Backdoor-B2D4G5-Tool-Use\", withExtension: \"mlmodel\")!\n",
    "        let compiledModelURL = try MLModel.compileModel(at: modelURL)\n",
    "        model = try MLModel(contentsOf: compiledModelURL)\n",
    "        \n",
    "        // Initialize tokenizer\n",
    "        tokenizer = NLTokenizer(unit: .word)\n",
    "    }\n",
    "    \n",
    "    func generateText(prompt: String, maxNewTokens: Int = 100) throws -> String {\n",
    "        // In a real implementation, you would use a proper tokenizer for the model\n",
    "        // This is a simplified example\n",
    "        \n",
    "        // Create input tensors\n",
    "        let inputIds: [Int32] = [bosTokenId] + tokenizeText(prompt) // Simplified tokenization\n",
    "        let inputLength = inputIds.count\n",
    "        \n",
    "        // Create attention mask (all 1s for this example)\n",
    "        let attentionMask = Array(repeating: Int32(1), count: inputLength)\n",
    "        \n",
    "        // Create MLMultiArray inputs\n",
    "        let inputIdsMultiArray = try MLMultiArray(shape: [1, NSNumber(value: inputLength)], dataType: .int32)\n",
    "        let attentionMaskMultiArray = try MLMultiArray(shape: [1, NSNumber(value: inputLength)], dataType: .int32)\n",
    "        \n",
    "        // Fill the input arrays\n",
    "        for i in 0..<inputLength {\n",
    "            inputIdsMultiArray[i] = NSNumber(value: inputIds[i])\n",
    "            attentionMaskMultiArray[i] = NSNumber(value: attentionMask[i])\n",
    "        }\n",
    "        \n",
    "        // Create model input\n",
    "        let modelInput = BackdoorB2D4G5ToolUseInput(\n",
    "            input_ids: inputIdsMultiArray,\n",
    "            attention_mask: attentionMaskMultiArray\n",
    "        )\n",
    "        \n",
    "        // Get model output\n",
    "        let prediction = try model.prediction(from: modelInput)\n",
    "        let logits = prediction.featureValue(for: \"logits\")!.multiArrayValue!\n",
    "        \n",
    "        // In a real implementation, you would:\n",
    "        // 1. Get the next token by finding the argmax of the last token's logits\n",
    "        // 2. Append it to the input sequence\n",
    "        // 3. Run the model again with the updated sequence\n",
    "        // 4. Repeat until EOS token or max length is reached\n",
    "        \n",
    "        // This is a placeholder for the actual generation logic\n",
    "        return \"Generated text would appear here\"\n",
    "    }\n",
    "    \n",
    "    private func tokenizeText(_ text: String) -> [Int32] {\n",
    "        // This is a placeholder for actual tokenization\n",
    "        // In a real implementation, you would use the model's tokenizer\n",
    "        return Array(repeating: Int32(0), count: 10)\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Save the Swift example\n",
    "swift_example_path = os.path.join(OUTPUT_DIR, \"BackdoorModelHandler.swift\")\n",
    "with open(swift_example_path, \"w\") as f:\n",
    "    f.write(swift_code)\n",
    "\n",
    "print(f\"Swift example code saved to {swift_example_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create a Python Helper for Using the CoreML Model\n",
    "\n",
    "Let's also create a Python helper to demonstrate how to use the CoreML model in Python applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python helper class\n",
    "python_code = \"\"\"\n",
    "import coremltools as ct\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BackdoorModelHelper:\n",
    "    def __init__(self, model_path, tokenizer_path):\n",
    "        \"\"\"\n",
    "        Initialize the helper with paths to the CoreML model and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the .mlmodel file\n",
    "            tokenizer_path (str): Path to the tokenizer files\n",
    "        \"\"\"\n",
    "        # Load the CoreML model\n",
    "        self.model = ct.models.MLModel(model_path)\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        # Set padding token if not set\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def generate_text(self, prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate text from a prompt using the CoreML model.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input prompt\n",
    "            max_new_tokens (int): Maximum number of new tokens to generate\n",
    "            temperature (float): Sampling temperature\n",
    "            top_p (float): Top-p sampling parameter\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated text\n",
    "        \"\"\"\n",
    "        # Tokenize the prompt\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"np\")\n",
    "        input_ids = inputs[\"input_ids\"].astype(np.int32)\n",
    "        attention_mask = inputs[\"attention_mask\"].astype(np.int32)\n",
    "        \n",
    "        # Initialize the generated sequence with the input\n",
    "        generated_ids = input_ids.copy()\n",
    "        \n",
    "        # Generate tokens one by one\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Prepare inputs for the model\n",
    "            model_inputs = {\n",
    "                \"input_ids\": generated_ids,\n",
    "                \"attention_mask\": np.ones_like(generated_ids, dtype=np.int32)\n",
    "            }\n",
    "            \n",
    "            # Run the model\n",
    "            outputs = self.model.predict(model_inputs)\n",
    "            logits = outputs[\"logits\"]\n",
    "            \n",
    "            # Get the logits for the last token\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Apply top-p sampling\n",
    "            sorted_logits, sorted_indices = np.sort(next_token_logits)[::-1], np.argsort(next_token_logits)[::-1]\n",
    "            cumulative_probs = np.cumsum(np.exp(sorted_logits) / np.sum(np.exp(sorted_logits)))\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n",
    "            sorted_indices_to_remove[0] = False\n",
    "            next_token_logits[sorted_indices[sorted_indices_to_remove]] = -float(\"Inf\")\n",
    "            \n",
    "            # Sample from the filtered distribution\n",
    "            probs = np.exp(next_token_logits) / np.sum(np.exp(next_token_logits))\n",
    "            next_token = np.random.choice(len(probs), p=probs)\n",
    "            \n",
    "            # If EOS token is generated, stop\n",
    "            if next_token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            # Add the next token to the generated sequence\n",
    "            generated_ids = np.concatenate([generated_ids, [[next_token]]], axis=1)\n",
    "        \n",
    "        # Decode the generated sequence\n",
    "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "# Example usage:\n",
    "# helper = BackdoorModelHelper(\"path/to/model.mlmodel\", \"path/to/tokenizer\")\n",
    "# generated_text = helper.generate_text(\"Hello, I am an AI assistant.\")\n",
    "# print(generated_text)\n",
    "\"\"\"\n",
    "\n",
    "# Save the Python example\n",
    "python_example_path = os.path.join(OUTPUT_DIR, \"backdoor_model_helper.py\")\n",
    "with open(python_example_path, \"w\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python example code saved to {python_example_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create a README for the CoreML Model\n",
    "\n",
    "Let's create a README file with instructions for using the CoreML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a README file\n",
    "readme_content = \"\"\"\n",
    "# Backdoor-B2D4G5-Tool-Use CoreML Model\n",
    "\n",
    "This directory contains the Backdoor-B2D4G5-Tool-Use model (originally Llama-3-Groq-8B-Tool-Use) converted to CoreML format for use on Apple devices.\n",
    "\n",
    "## Files\n",
    "\n",
    "- `Backdoor-B2D4G5-Tool-Use.mlmodel`: The quantized CoreML model (8-bit quantization)\n",
    "- `Backdoor-B2D4G5-Tool-Use-original.mlmodel`: The original (non-quantized) CoreML model\n",
    "- `tokenizer_config.json`: Configuration for the tokenizer\n",
    "- `BackdoorModelHandler.swift`: Example Swift code for using the model\n",
    "- `backdoor_model_helper.py`: Example Python code for using the model\n",
    "\n",
    "## Model Information\n",
    "\n",
    "- **Original Model**: Llama-3-Groq-8B-Tool-Use\n",
    "- **Renamed As**: Backdoor-B2D4G5-Tool-Use\n",
    "- **Model Size**: 8 billion parameters\n",
    "- **Architecture**: Transformer-based language model\n",
    "- **Quantization**: 8-bit linear quantization\n",
    "- **Minimum Deployment Target**: iOS 16+\n",
    "\n",
    "## Using the Model in Swift\n",
    "\n",
    "1. Add the `.mlmodel` file to your Xcode project\n",
    "2. Xcode will automatically generate a Swift class for the model\n",
    "3. Use the example code in `BackdoorModelHandler.swift` as a starting point\n",
    "\n",
    "## Using the Model in Python\n",
    "\n",
    "1. Install the required dependencies: `pip install coremltools numpy transformers`\n",
    "2. Use the example code in `backdoor_model_helper.py` as a starting point\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "- The model is optimized for Apple Neural Engine but will also run on CPU and GPU\n",
    "- The quantized model is significantly smaller but may have slightly reduced accuracy\n",
    "- For best performance, use the model on devices with Apple Silicon (M1/M2/M3 or newer)\n",
    "- The model supports variable sequence lengths up to 2048 tokens\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The model uses the same tokenizer as the original Llama-3-Groq-8B-Tool-Use model. You'll need to use the Hugging Face transformers library to load the tokenizer from the original model or use a compatible tokenizer.\n",
    "\n",
    "## License\n",
    "\n",
    "This model is provided under the Apache 2.0 license. Please refer to the original model's license for any additional terms and conditions.\n",
    "\"\"\"\n",
    "\n",
    "# Save the README\n",
    "readme_path = os.path.join(OUTPUT_DIR, \"README.md\")\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"README saved to {readme_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully converted the Backdoor-B2D4G5-Tool-Use (Llama-3-Groq-8B-Tool-Use) model to CoreML format. Here's a summary of what we've accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the output directory\n",
    "print(\"Files created:\")\n",
    "for file in os.listdir(OUTPUT_DIR):\n",
    "    file_path = os.path.join(OUTPUT_DIR, file)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"- {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "print(\"\\nConversion process complete!\")\n",
    "print(\"The model has been successfully converted to CoreML format and is ready for use on Apple devices.\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download the converted model files from the Kaggle output directory\")\n",
    "print(\"2. Integrate the model into your iOS, macOS, or other Apple platform application\")\n",
    "print(\"3. Use the provided example code as a starting point for your implementation\")\n",
    "print(\"4. Test the model thoroughly on your target devices\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}