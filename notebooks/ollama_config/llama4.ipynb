{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollama-header"
      },
      "source": [
        "# Backdoor AI - Ollama on Google Colab\n",
        "\n",
        "This notebook helps you run Ollama on Google Colab to use with your Backdoor AI application. You can install and run Ollama models (including Llama4) directly in Colab, then connect your Backdoor AI app to it.\n",
        "\n",
        "## How it works\n",
        "\n",
        "1. This notebook will install Ollama on this Colab instance\n",
        "2. You'll select and download models (Llama4 or others)\n",
        "3. We'll set up Cloudflared to create a secure tunnel to your Ollama instance\n",
        "4. You'll get a URL to use in your Backdoor AI settings\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "requirements-section"
      },
      "source": [
        "## 1. Set up environment\n",
        "\n",
        "First, let's install the required packages. We'll need Ollama and Cloudflared for tunneling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-requirements"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install cloudflared for tunneling\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install -q requests pyngrok httpx\n",
        "\n",
        "# Set up directories\n",
        "!mkdir -p /tmp/ollama/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start-ollama"
      },
      "source": [
        "## 2. Start Ollama server\n",
        "\n",
        "Now we'll start the Ollama server in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-ollama-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Start Ollama server in background\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Wait for Ollama to start\n",
        "print(\"Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if Ollama is running\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/version\")\n",
        "    if response.status_code == 200:\n",
        "        print(f\"‚úÖ Ollama is running! Version: {response.json().get('version')}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Ollama returned unexpected status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to connect to Ollama: {e}\")\n",
        "    print(\"Trying to start again...\")\n",
        "    # Kill the previous process if it exists\n",
        "    if ollama_process:\n",
        "        ollama_process.terminate()\n",
        "        time.sleep(2)\n",
        "    # Try starting again\n",
        "    !ollama serve &\n",
        "    time.sleep(5)\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/version\")\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ Second attempt succeeded! Ollama is running. Version: {response.json().get('version')}\")\n",
        "    except:\n",
        "        print(\"‚ùå Failed to start Ollama after multiple attempts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-section"
      },
      "source": [
        "## 3. Choose and download a model\n",
        "\n",
        "Now, let's download a model. We recommend Llama4 models for best performance or compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-selection"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Define recommended models\n",
        "recommended_models = [\n",
        "    {\"name\": \"Llama4 (70B - Best quality)\", \"id\": \"llama4:latest\", \"size\": \"~70GB\"},\n",
        "    {\"name\": \"Llama4 (8B - Faster)\", \"id\": \"llama4-8b:latest\", \"size\": \"~8GB\"},\n",
        "    {\"name\": \"Llama4 Code (Code-specialized)\", \"id\": \"llama4-code:latest\", \"size\": \"~70GB\"},\n",
        "    {\"name\": \"Llama4 Tiny (Smallest, fastest)\", \"id\": \"llama4-tiny:latest\", \"size\": \"~1.5GB\"},\n",
        "    {\"name\": \"Mistral (7B)\", \"id\": \"mistral:latest\", \"size\": \"~7GB\"},\n",
        "    {\"name\": \"Gemma (7B)\", \"id\": \"gemma:latest\", \"size\": \"~7GB\"},\n",
        "    {\"name\": \"Neural Chat (7B)\", \"id\": \"neural-chat:latest\", \"size\": \"~7GB\"},\n",
        "    {\"name\": \"Custom model (enter below)\", \"id\": \"custom\", \"size\": \"varies\"}\n",
        "]\n",
        "\n",
        "# Create dropdown for model selection\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[(f\"{model['name']} ({model['size']})\", model['id']) for model in recommended_models],\n",
        "    description='Select model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "# Create text field for custom model\n",
        "custom_model = widgets.Text(\n",
        "    description='Custom model:',\n",
        "    placeholder='Enter model name (e.g., llama4-7b:latest)',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%', display='none')\n",
        ")\n",
        "\n",
        "# Download button\n",
        "download_button = widgets.Button(description='Download Model', button_style='primary')\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Function to handle model dropdown change\n",
        "def on_model_change(change):\n",
        "    if change['new'] == 'custom':\n",
        "        custom_model.layout.display = 'block'\n",
        "    else:\n",
        "        custom_model.layout.display = 'none'\n",
        "\n",
        "# Function to download model\n",
        "def download_model(b):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        model_id = model_dropdown.value\n",
        "        \n",
        "        if model_id == 'custom':\n",
        "            if not custom_model.value.strip():\n",
        "                print(\"‚ö†Ô∏è Please enter a custom model name!\")\n",
        "                return\n",
        "            model_id = custom_model.value.strip()\n",
        "        \n",
        "        print(f\"üöÄ Downloading model: {model_id}\")\n",
        "        print(\"This may take a while depending on the model size and your internet connection...\")\n",
        "        print(\"You'll see progress below. Please don't interrupt the process.\")\n",
        "        \n",
        "        # Run ollama pull command\n",
        "        process = subprocess.Popen(\n",
        "            [\"ollama\", \"pull\", model_id],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "        \n",
        "        # Show live output\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                print(output.strip())\n",
        "        \n",
        "        return_code = process.poll()\n",
        "        if return_code == 0:\n",
        "            print(f\"‚úÖ Model {model_id} downloaded successfully!\")\n",
        "            # List available models\n",
        "            print(\"\\nüìã Available models:\")\n",
        "            !ollama list\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to download model {model_id}. Return code: {return_code}\")\n",
        "\n",
        "# Connect events\n",
        "model_dropdown.observe(on_model_change, names='value')\n",
        "download_button.on_click(download_model)\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.HTML(\"<h3>Select a model to download:</h3>\"))\n",
        "display(model_dropdown)\n",
        "display(custom_model)\n",
        "display(download_button)\n",
        "display(output_area)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-model"
      },
      "source": [
        "## 4. Test the model\n",
        "\n",
        "Let's make sure the model works by asking it a simple question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-model-code"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Function to test a model\n",
        "def test_model(model_id, prompt=\"Hi, I'm testing if you're working properly. Please give a brief greeting.\"):\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"response\": result.get(\"message\", {}).get(\"content\", \"No content returned\")\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": f\"Server returned status code {response.status_code}: {response.text}\"\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Get available models\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "    if response.status_code == 200:\n",
        "        models = response.json().get(\"models\", [])\n",
        "        model_options = [(model.get(\"name\"), model.get(\"name\")) for model in models]\n",
        "    else:\n",
        "        model_options = [(\"No models found\", \"\")]\n",
        "except Exception as e:\n",
        "    model_options = [(f\"Error: {str(e)}\", \"\")]\n",
        "\n",
        "# Create widgets\n",
        "test_model_dropdown = widgets.Dropdown(\n",
        "    options=model_options,\n",
        "    description='Model to test:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "test_prompt = widgets.Textarea(\n",
        "    value=\"Hi, I'm testing if you're working properly. Please give a brief greeting.\",\n",
        "    description='Prompt:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='80%', height='100px')\n",
        ")\n",
        "\n",
        "test_button = widgets.Button(description='Test Model', button_style='success')\n",
        "test_output = widgets.Output()\n",
        "\n",
        "# Function to test model\n",
        "def on_test_button_click(b):\n",
        "    with test_output:\n",
        "        clear_output()\n",
        "        if not test_model_dropdown.value:\n",
        "            print(\"‚ö†Ô∏è Please select a model to test!\")\n",
        "            return\n",
        "        \n",
        "        print(f\"üîç Testing model '{test_model_dropdown.value}' with prompt: \\n{test_prompt.value}\\n\")\n",
        "        print(\"Waiting for response...\")\n",
        "        \n",
        "        result = test_model(test_model_dropdown.value, test_prompt.value)\n",
        "        \n",
        "        if result[\"success\"]:\n",
        "            print(\"\\n‚úÖ Model responded successfully!\\n\")\n",
        "            print(\"Response:\")\n",
        "            print(\"-----------------------------------\")\n",
        "            print(result[\"response\"])\n",
        "            print(\"-----------------------------------\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå Error testing model: {result['error']}\")\n",
        "\n",
        "# Connect events\n",
        "test_button.on_click(on_test_button_click)\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.HTML(\"<h3>Test your model:</h3>\"))\n",
        "display(test_model_dropdown)\n",
        "display(test_prompt)\n",
        "display(test_button)\n",
        "display(test_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-tunnel"
      },
      "source": [
        "## 5. Set up a tunnel to access your Ollama instance\n",
        "\n",
        "Now we'll set up a Cloudflare tunnel so your Backdoor AI application can access this Ollama instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunnel-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Function to run cloudflared tunnel in a separate thread\n",
        "def run_tunnel():\n",
        "    global tunnel_process, tunnel_url\n",
        "    tunnel_process = subprocess.Popen(\n",
        "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "    \n",
        "    # Extract tunnel URL\n",
        "    tunnel_url = None\n",
        "    url_pattern = re.compile(r'https://[\\w.-]+\\.trycloudflare\\.com')\n",
        "    \n",
        "    while True:\n",
        "        line = tunnel_process.stdout.readline()\n",
        "        if not line and tunnel_process.poll() is not None:\n",
        "            break\n",
        "        \n",
        "        match = url_pattern.search(line)\n",
        "        if match and not tunnel_url:\n",
        "            tunnel_url = match.group(0)\n",
        "            print(f\"‚úÖ Tunnel established at: {tunnel_url}\")\n",
        "            # Update tunnel info\n",
        "            tunnel_info.value = f\"<div style='padding: 10px; background-color: #e6ffe6; border-radius: 5px;'><b>‚úÖ Your Ollama API is accessible at:</b><br><code>{tunnel_url}</code><br><br>Use this URL in your Backdoor AI settings as the Ollama API Base URL.<br>Keep this notebook running while you're using Ollama with your app!</div>\"\n",
        "\n",
        "# Create widgets for tunnel info\n",
        "tunnel_button = widgets.Button(description='Start Tunnel', button_style='info')\n",
        "tunnel_info = widgets.HTML(\"\")\n",
        "\n",
        "# Function to start tunnel\n",
        "def on_tunnel_button_click(b):\n",
        "    b.description = \"Starting...\"\n",
        "    b.disabled = True\n",
        "    tunnel_info.value = \"<div style='padding: 10px; background-color: #fff3e6; border-radius: 5px;'>‚è≥ Creating secure tunnel to Ollama... (this may take a moment)</div>\"\n",
        "    \n",
        "    # Start tunnel in separate thread\n",
        "    thread = threading.Thread(target=run_tunnel)\n",
        "    thread.daemon = True\n",
        "    thread.start()\n",
        "    \n",
        "    # Check for tunnel URL\n",
        "    attempts = 0\n",
        "    while attempts < 10 and not tunnel_url:\n",
        "        time.sleep(1)\n",
        "        attempts += 1\n",
        "    \n",
        "    if not tunnel_url:\n",
        "        tunnel_info.value = \"<div style='padding: 10px; background-color: #ffe6e6; border-radius: 5px;'>‚ùå Failed to create tunnel. Check the output below for details.</div>\"\n",
        "        b.description = \"Try Again\"\n",
        "        b.disabled = False\n",
        "\n",
        "# Connect events\n",
        "tunnel_button.on_click(on_tunnel_button_click)\n",
        "\n",
        "# Initialize global variables\n",
        "tunnel_process = None\n",
        "tunnel_url = None\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.HTML(\"<h3>Create a secure tunnel to your Ollama instance:</h3>\"))\n",
        "display(widgets.HTML(\"<p>This will create a public URL that you can use to connect your Backdoor AI application to this Ollama instance.</p>\"))\n",
        "display(tunnel_button)\n",
        "display(tunnel_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "backdoor-instructions"
      },
      "source": [
        "## 6. Connect Backdoor AI to your Ollama instance\n",
        "\n",
        "Once you have your tunnel URL, follow these steps to connect Backdoor AI to your Ollama instance:\n",
        "\n",
        "1. Go to your Backdoor AI settings page\n",
        "2. Select \"Ollama (Remote)\" as your LLM provider\n",
        "3. In the \"Ollama API URL\" field, enter the tunnel URL from above\n",
        "4. In the \"Ollama Model\" dropdown, select the model you downloaded\n",
        "5. Click \"Save Settings\"\n",
        "\n",
        "**Important Notes:**\n",
        "- Keep this notebook running as long as you're using Ollama with your app\n",
        "- The tunnel URL will change each time you restart this notebook\n",
        "- Google Colab sessions have limited runtime (a few hours for free tier)\n",
        "- Your model downloads will be lost when the Colab session ends\n",
        "\n",
        "If you want a more permanent solution, consider setting up Ollama on your own machine or a cloud server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "closing-notes"
      },
      "source": [
        "## Additional Information\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Ollama not starting**: Try restarting the runtime (Runtime > Restart runtime) and run all cells again\n",
        "2. **Model download failing**: Check your internet connection and try a smaller model\n",
        "3. **Tunnel not working**: Make sure Ollama is running properly, then try starting the tunnel again\n",
        "\n",
        "### Keeping Colab Active\n",
        "\n",
        "Google Colab sessions will disconnect after periods of inactivity. To keep your session active:\n",
        "- Keep the browser tab open\n",
        "- Consider using browser extensions that simulate activity\n",
        "\n",
        "### Shutting Down\n",
        "\n",
        "When you're done, remember to:\n",
        "1. Switch your Backdoor AI back to another provider if needed\n",
        "2. Close this notebook\n",
        "\n",
        "### For Production Use\n",
        "\n",
        "For a more reliable solution, consider:\n",
        "- Running Ollama on your own hardware or a cloud VM\n",
        "- Setting up proper authentication and security measures\n",
        "- Using persistent storage for model files"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
