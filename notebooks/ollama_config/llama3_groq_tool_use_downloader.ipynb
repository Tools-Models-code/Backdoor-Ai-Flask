{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Backdoor AI - Llama3 Groq Tool Use Model Downloader (Hugging Face)\n",
        "\n",
        "This notebook downloads the Llama-3-Groq-8B-Tool-Use model from Hugging Face and zips it for easy download. This notebook is designed to work with Google Colab.\n",
        "\n",
        "## How it works\n",
        "\n",
        "1. Check if we're running in Google Colab\n",
        "2. Optimize the Colab environment for large models\n",
        "3. Install Hugging Face Hub and necessary dependencies\n",
        "4. Download the Llama-3-Groq-8B-Tool-Use model from Hugging Face\n",
        "5. Zip the model files for easy download\n",
        "6. Provide a download link for the zipped model\n",
        "7. Upload the zipped model to Dropbox (optional)\n",
        "\n",
        "## Dropbox Integration\n",
        "\n",
        "This notebook includes functionality to upload the zipped model directly to your Dropbox account. The Dropbox integration uses the following credentials:\n",
        "\n",
        "- **App Key**: 2bi422xpd3xd962\n",
        "- **App Secret**: j3yx0b41qdvfu86\n",
        "- **Refresh Token**: RvyL03RE5qAAAAAAAAAAAVMVebvE7jDx8Okd0ploMzr85c6txvCRXpJAt30mxrKF\n",
        "\n",
        "The model will be uploaded to the `/llama3-groq-tool-use-8b/` folder in your Dropbox account.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-check"
      },
      "source": [
        "## Check if running in Google Colab\n",
        "\n",
        "First, let's make sure we're running in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-colab"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if not IN_COLAB:\n",
        "    print(\"\u26a0\ufe0f This notebook is designed to run in Google Colab. Some features may not work as expected.\")\n",
        "else:\n",
        "    print(\"\u2705 Running in Google Colab!\")\n",
        "    \n",
        "    # Import Google Colab specific modules\n",
        "    from google.colab import drive, files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-deps"
      },
      "source": [
        "## Memory Optimization for Large Models\n",
        "\n",
        "First, let's clear up disk space and optimize memory to ensure we have enough resources for large models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-dependencies"
      },
      "outputs": [],
      "source": [
        "# Memory optimization functions\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import gc\n",
        "import time\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Install required packages first\n",
        "!pip install -q psutil tqdm\n",
        "import psutil\n",
        "\n",
        "def clear_disk_space():\n",
        "    \"\"\"Clean up disk space by removing unnecessary files.\"\"\"\n",
        "    print(\"\ud83e\uddf9 Cleaning up disk space...\")\n",
        "    \n",
        "    # Clean apt cache\n",
        "    subprocess.run(\"apt-get clean\", shell=True)\n",
        "    \n",
        "    # Remove unnecessary packages\n",
        "    subprocess.run(\"apt-get -y autoremove\", shell=True)\n",
        "    \n",
        "    # Clean pip cache\n",
        "    subprocess.run(\"rm -rf ~/.cache/pip\", shell=True)\n",
        "    \n",
        "    # Remove temporary files\n",
        "    temp_dirs = ['/tmp', '/var/tmp']\n",
        "    for temp_dir in temp_dirs:\n",
        "        if os.path.exists(temp_dir):\n",
        "            try:\n",
        "                for item in os.listdir(temp_dir):\n",
        "                    item_path = os.path.join(temp_dir, item)\n",
        "                    # Skip our ollama directories\n",
        "                    if item.startswith('ollama') or item.startswith('backdoor'):\n",
        "                        continue\n",
        "                    \n",
        "                    try:\n",
        "                        if os.path.isdir(item_path):\n",
        "                            shutil.rmtree(item_path)\n",
        "                        else:\n",
        "                            os.remove(item_path)\n",
        "                    except Exception as e:\n",
        "                        pass  # Skip files that can't be removed\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not clean {temp_dir}: {e}\")\n",
        "    \n",
        "    # Remove unused Docker images/containers if Docker is installed\n",
        "    try:\n",
        "        subprocess.run(\"docker system prune -af\", shell=True, stderr=subprocess.DEVNULL)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"\u2705 Disk cleanup complete!\")\n",
        "    \n",
        "    # Show disk space\n",
        "    show_disk_usage()\n",
        "\n",
        "def show_disk_usage():\n",
        "    \"\"\"Show current disk usage.\"\"\"\n",
        "    try:\n",
        "        df_output = subprocess.check_output(\"df -h /\", shell=True, text=True)\n",
        "        print(\"\\n\ud83d\udcca Disk Space Available:\")\n",
        "        for line in df_output.split('\\n'):\n",
        "            print(line)\n",
        "    except:\n",
        "        print(\"Could not retrieve disk usage information\")\n",
        "\n",
        "def show_memory_usage():\n",
        "    \"\"\"Show current memory usage.\"\"\"\n",
        "    try:\n",
        "        memory = psutil.virtual_memory()\n",
        "        total_gb = memory.total / (1024 ** 3)\n",
        "        available_gb = memory.available / (1024 ** 3)\n",
        "        used_gb = memory.used / (1024 ** 3)\n",
        "        percent = memory.percent\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcca Memory Usage:\")\n",
        "        print(f\"Total Memory: {total_gb:.2f} GB\")\n",
        "        print(f\"Available: {available_gb:.2f} GB\")\n",
        "        print(f\"Used: {used_gb:.2f} GB ({percent}%)\")\n",
        "    except:\n",
        "        print(\"Could not retrieve memory usage information\")\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear Python memory.\"\"\"\n",
        "    gc.collect()\n",
        "    torch_available = False\n",
        "    \n",
        "    try:\n",
        "        import torch\n",
        "        torch_available = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    if torch_available:\n",
        "        try:\n",
        "            import torch\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"\u2705 PyTorch CUDA cache cleared\")\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    print(\"\u2705 Python memory cleared\")\n",
        "\n",
        "# Run optimization process\n",
        "print(\"\ud83d\ude80 Optimizing environment for large language models...\")\n",
        "clear_disk_space()\n",
        "clear_memory()\n",
        "\n",
        "# Set environment variables for improved performance\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "\n",
        "# Show current resource usage\n",
        "show_memory_usage()\n",
        "show_disk_usage()\n",
        "\n",
        "print(\"\\n\u2705 Optimization complete! Ready to continue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-model-header"
      },
      "source": [
        "## Install Hugging Face Hub\n",
        "\n",
        "Now let's install Hugging Face Hub and other required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-model"
      },
      "outputs": [],
      "source": [
        "# Install Hugging Face Hub and other dependencies\n",
        "!pip install -q huggingface_hub tqdm requests ipywidgets\n",
        "\n",
        "# Set up directories\n",
        "!mkdir -p /content/llama3-groq-tool-use-8b\n",
        "\n",
        "print(\"\u2705 Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf-login-header"
      },
      "source": [
        "## Login to Hugging Face\n",
        "\n",
        "Now we'll login to Hugging Face using the provided API token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf-login"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Set the Hugging Face API token\n",
        "HF_TOKEN = \"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\"  # API token provided by user\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=HF_TOKEN)\n",
        "print(\"\u2705 Successfully logged in to Hugging Face!\")\n",
        "\n",
        "# Link to allow this token in GitHub\n",
        "from IPython.display import display, HTML\n",
        "allow_token_url = \"https://github.com/backdoor-testing-tools/Backdoor-Ai-Flask/security/secret-scanning/unblock-secret/2wf2rCQIoNHHGW5bHh1H5vMXjOP\"\n",
        "display(HTML(f'<a href=\"{allow_token_url}\" target=\"_blank\">Click here to allow this token in GitHub</a>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-zip-header"
      },
      "source": [
        "## Download the Llama-3-Groq-8B-Tool-Use Model\n",
        "\n",
        "Now let's download the Llama-3-Groq-8B-Tool-Use model from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-zip"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from huggingface_hub import snapshot_download\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set the model repository ID\n",
        "repo_id = \"Groq/Llama-3-Groq-8B-Tool-Use\"\n",
        "output_dir = \"/content/llama3-groq-tool-use-8b\"\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Function to download model\n",
        "def download_model():\n",
        "    with output_area:\n",
        "        print(f\"\ud83d\ude80 Downloading {repo_id} model from Hugging Face...\")\n",
        "        print(\"This may take a while depending on your internet connection...\")\n",
        "        print(\"You'll see progress below. Please don't interrupt the process.\")\n",
        "        \n",
        "        try:\n",
        "            # Download the model\n",
        "            snapshot_download(\n",
        "                repo_id=repo_id,\n",
        "                local_dir=output_dir,\n",
        "                local_dir_use_symlinks=False,  # Don't use symlinks to ensure all files are copied\n",
        "                token=HF_TOKEN\n",
        "            )\n",
        "            \n",
        "            print(f\"\u2705 Model {repo_id} downloaded successfully to {output_dir}!\")\n",
        "            \n",
        "            # List downloaded files\n",
        "            print(\"\\n\ud83d\udccb Downloaded model files:\")\n",
        "            !ls -la {output_dir}\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Failed to download model: {e}\")\n",
        "\n",
        "# Download the model\n",
        "download_model()\n",
        "display(output_area)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup-header"
      },
      "source": [
        "## Zip the Model Files\n",
        "\n",
        "Now let's zip the model files for easy download using optimized compression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import zipfile\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import multiprocessing\n",
        "\n",
        "# Define the model name and paths\n",
        "model_name = \"llama3-groq-tool-use-8b\"\n",
        "model_dir = \"/content/llama3-groq-tool-use-8b\"\n",
        "zip_dir = \"/content\"\n",
        "zip_file = f\"{zip_dir}/{model_name}.zip\"\n",
        "\n",
        "# Function to get all files in a directory recursively\n",
        "def get_all_files(directory):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_paths.append(file_path)\n",
        "    return file_paths\n",
        "\n",
        "# Optimized zip function with progress bar\n",
        "def optimized_zip(source_dir, output_zip, compression=zipfile.ZIP_DEFLATED):\n",
        "    # Get all files to zip\n",
        "    all_files = get_all_files(source_dir)\n",
        "    total_files = len(all_files)\n",
        "    \n",
        "    if total_files == 0:\n",
        "        print(\"No files found to zip!\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"Found {total_files} files to zip\")\n",
        "    \n",
        "    # Create a new zip file\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create progress bar\n",
        "    pbar = tqdm(total=total_files, desc=\"Zipping files\", unit=\"files\")\n",
        "    \n",
        "    # Create zip file with optimized settings\n",
        "    with zipfile.ZipFile(output_zip, 'w', compression=compression, allowZip64=True) as zipf:\n",
        "        # Process files in batches for better memory management\n",
        "        for file_path in all_files:\n",
        "            # Create the archive path (relative to source_dir)\n",
        "            arc_path = os.path.relpath(file_path, source_dir)\n",
        "            try:\n",
        "                # Add file to zip\n",
        "                zipf.write(file_path, arc_path)\n",
        "                pbar.update(1)\n",
        "            except Exception as e:\n",
        "                print(f\"Error adding {file_path} to zip: {e}\")\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Zipping completed in {elapsed_time:.2f} seconds\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Alternative method using shutil with progress tracking\n",
        "def shutil_zip_with_progress(source_dir, output_zip_base):\n",
        "    # Count files for progress tracking\n",
        "    all_files = get_all_files(source_dir)\n",
        "    total_files = len(all_files)\n",
        "    \n",
        "    if total_files == 0:\n",
        "        print(\"No files found to zip!\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"Found {total_files} files to zip using shutil\")\n",
        "    \n",
        "    # Create a progress bar that will update based on file count\n",
        "    pbar = tqdm(total=total_files, desc=\"Preparing to zip\", unit=\"files\")\n",
        "    \n",
        "    # Define a callback function to update progress\n",
        "    def progress_callback(progress_pbar):\n",
        "        def callback(chunk):\n",
        "            # This is called for each file, but we don't know exactly when\n",
        "            # So we'll update by small increments\n",
        "            progress_pbar.update(1)\n",
        "        return callback\n",
        "    \n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Use shutil.make_archive which is optimized for speed\n",
        "    try:\n",
        "        # Unfortunately shutil doesn't provide progress callbacks\n",
        "        # So we'll just show the progress bar as an indicator\n",
        "        pbar.set_description(\"Zipping files (please wait)\")\n",
        "        shutil.make_archive(output_zip_base, 'zip', source_dir)\n",
        "        pbar.update(total_files)  # Complete the progress bar\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating zip: {e}\")\n",
        "        pbar.close()\n",
        "        return False\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Zipping completed in {elapsed_time:.2f} seconds\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Verify the model directory exists and has files\n",
        "if not os.path.exists(model_dir):\n",
        "    print(f\"\u274c Model directory {model_dir} does not exist. Please download the model first.\")\n",
        "else:\n",
        "    # Count files in the model directory\n",
        "    file_count = sum([len(files) for _, _, files in os.walk(model_dir)])\n",
        "    print(f\"Found {file_count} files in the model directory.\")\n",
        "    \n",
        "    # Zip the model directory\n",
        "    print(f\"\\nZipping model files to {zip_file}...\")\n",
        "    try:\n",
        "        # Remove existing zip file if it exists\n",
        "        if os.path.exists(zip_file):\n",
        "            os.remove(zip_file)\n",
        "            print(f\"Removed existing zip file.\")\n",
        "        \n",
        "        # Choose the best compression method based on speed vs size tradeoff\n",
        "        # ZIP_STORED (no compression, fastest)\n",
        "        # ZIP_DEFLATED (good compression, decent speed)\n",
        "        # ZIP_LZMA (best compression, slowest)\n",
        "        compression_method = zipfile.ZIP_DEFLATED\n",
        "        compression_name = \"DEFLATED\"\n",
        "        \n",
        "        print(f\"Using {compression_name} compression method for optimal speed/size balance\")\n",
        "        \n",
        "        # Use the faster shutil method with progress tracking\n",
        "        output_zip_base = f\"{zip_dir}/{model_name}\"\n",
        "        success = shutil_zip_with_progress(model_dir, output_zip_base)\n",
        "        \n",
        "        # Get the size of the zip file\n",
        "        if success and os.path.exists(zip_file):\n",
        "            zip_size = os.path.getsize(zip_file) / (1024 * 1024 * 1024)  # Convert to GB\n",
        "            print(f\"\u2705 Model zipped successfully to {zip_file}\")\n",
        "            print(f\"Zip file size: {zip_size:.2f} GB\")\n",
        "            \n",
        "            # Calculate compression ratio\n",
        "            original_size = sum(os.path.getsize(os.path.join(dirpath, filename)) \n",
        "                              for dirpath, _, filenames in os.walk(model_dir) \n",
        "                              for filename in filenames) / (1024 * 1024 * 1024)  # Convert to GB\n",
        "            \n",
        "            compression_ratio = (original_size / zip_size) if zip_size > 0 else 0\n",
        "            print(f\"Original size: {original_size:.2f} GB\")\n",
        "            print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
        "        else:\n",
        "            print(f\"\u274c Failed to create zip file at {zip_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error zipping model files: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-section"
      },
      "source": [
        "## Download the Zipped Model\n",
        "\n",
        "Now you can download the zipped model to your local machine. Click the button below to start the download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-code"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "if IN_COLAB:\n",
        "    if os.path.exists(zip_file):\n",
        "        print(\"Preparing download...\")\n",
        "        from google.colab import files\n",
        "        \n",
        "        # Create a download button\n",
        "        download_button = widgets.Button(\n",
        "            description=\"Download Model\",\n",
        "            button_style=\"success\",\n",
        "            icon=\"download\"\n",
        "        )\n",
        "        \n",
        "        def on_button_click(b):\n",
        "            files.download(zip_file)\n",
        "        \n",
        "        download_button.on_click(on_button_click)\n",
        "        display(download_button)\n",
        "        \n",
        "        print(\"\u2705 Click the button above to download the model. If the download doesn't start automatically, check your browser's download manager.\")\n",
        "        print(f\"Model zip file size: {os.path.getsize(zip_file) / (1024 * 1024 * 1024):.2f} GB\")\n",
        "    else:\n",
        "        print(f\"\u274c Zip file {zip_file} does not exist. Please run the zipping cell first.\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f Not running in Colab. The zipped model is available at {zip_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dropbox-section"
      },
      "source": [
        "## Upload to Dropbox\n",
        "\n",
        "You can also upload the zipped model directly to your Dropbox account using the Dropbox API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dropbox-code"
      },
      "outputs": [],
      "source": [
        "# Install Dropbox SDK\n",
        "!pip install -q dropbox\n",
        "\n",
        "import os\n",
        "import dropbox\n",
        "from dropbox.exceptions import ApiError, AuthError\n",
        "from dropbox.oauth import DropboxOAuth2Flow\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "# Dropbox credentials\n",
        "DROPBOX_APP_KEY = \"2bi422xpd3xd962\"\n",
        "DROPBOX_APP_SECRET = \"j3yx0b41qdvfu86\"\n",
        "DROPBOX_REFRESH_TOKEN = \"RvyL03RE5qAAAAAAAAAAAVMVebvE7jDx8Okd0ploMzr85c6txvCRXpJAt30mxrKF\"\n",
        "\n",
        "# Function to get an access token using the refresh token\n",
        "def get_access_token(app_key, app_secret, refresh_token):\n",
        "    try:\n",
        "        # Create a Dropbox client\n",
        "        dbx = dropbox.Dropbox(\n",
        "            app_key=app_key,\n",
        "            app_secret=app_secret,\n",
        "            oauth2_refresh_token=refresh_token\n",
        "        )\n",
        "        # Test the connection\n",
        "        dbx.users_get_current_account()\n",
        "        print(\"\u2705 Successfully connected to Dropbox!\")\n",
        "        return dbx\n",
        "    except AuthError as e:\n",
        "        print(f\"\u274c Authentication error: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error connecting to Dropbox: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to upload a file to Dropbox with progress tracking\n",
        "def upload_to_dropbox(dbx, file_path, dropbox_path):\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    chunk_size = 4 * 1024 * 1024  # 4MB chunks\n",
        "    \n",
        "    print(f\"Uploading {file_path} to Dropbox as {dropbox_path}\")\n",
        "    print(f\"File size: {file_size / (1024 * 1024 * 1024):.2f} GB\")\n",
        "    \n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            if file_size <= chunk_size:\n",
        "                # Small file, upload in one go\n",
        "                print(\"Uploading small file in one go...\")\n",
        "                dbx.files_upload(f.read(), dropbox_path, mode=dropbox.files.WriteMode.overwrite)\n",
        "            else:\n",
        "                # Large file, use chunked upload\n",
        "                print(\"Using chunked upload for large file...\")\n",
        "                upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))\n",
        "                cursor = dropbox.files.UploadSessionCursor(\n",
        "                    session_id=upload_session_start_result.session_id,\n",
        "                    offset=f.tell()\n",
        "                )\n",
        "                commit = dropbox.files.CommitInfo(\n",
        "                    path=dropbox_path,\n",
        "                    mode=dropbox.files.WriteMode.overwrite\n",
        "                )\n",
        "                \n",
        "                # Create a progress bar\n",
        "                pbar = tqdm(total=file_size, unit='B', unit_scale=True)\n",
        "                pbar.update(chunk_size)\n",
        "                \n",
        "                # Upload the file in chunks\n",
        "                while f.tell() < file_size:\n",
        "                    if (file_size - f.tell()) <= chunk_size:\n",
        "                        # Last chunk\n",
        "                        dbx.files_upload_session_finish(\n",
        "                            f.read(chunk_size),\n",
        "                            cursor,\n",
        "                            commit\n",
        "                        )\n",
        "                        pbar.update(file_size - pbar.n)\n",
        "                    else:\n",
        "                        # Not the last chunk\n",
        "                        dbx.files_upload_session_append_v2(\n",
        "                            f.read(chunk_size),\n",
        "                            cursor\n",
        "                        )\n",
        "                        cursor.offset = f.tell()\n",
        "                        pbar.update(chunk_size)\n",
        "                \n",
        "                pbar.close()\n",
        "        \n",
        "        print(f\"\u2705 Successfully uploaded {file_path} to Dropbox as {dropbox_path}!\")\n",
        "        \n",
        "        # Create a shared link\n",
        "        try:\n",
        "            shared_link = dbx.sharing_create_shared_link_with_settings(dropbox_path)\n",
        "            print(f\"\\n\ud83d\udd17 Shared link: {shared_link.url}\")\n",
        "        except ApiError as e:\n",
        "            # Check if the error is because a shared link already exists\n",
        "            if e.error.is_shared_link_already_exists():\n",
        "                shared_links = dbx.sharing_list_shared_links(path=dropbox_path).links\n",
        "                if shared_links:\n",
        "                    print(f\"\\n\ud83d\udd17 Shared link: {shared_links[0].url}\")\n",
        "            else:\n",
        "                print(f\"\\n\u26a0\ufe0f Could not create shared link: {e}\")\n",
        "        \n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error uploading file to Dropbox: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create a button to upload to Dropbox\n",
        "upload_button = widgets.Button(\n",
        "    description=\"Upload to Dropbox\",\n",
        "    button_style=\"info\",\n",
        "    icon=\"cloud-upload\"\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_upload_click(b):\n",
        "    with output_area:\n",
        "        if not os.path.exists(zip_file):\n",
        "            print(f\"\u274c Zip file {zip_file} does not exist. Please run the zipping cell first.\")\n",
        "            return\n",
        "        \n",
        "        # Get a Dropbox client\n",
        "        dbx = get_access_token(DROPBOX_APP_KEY, DROPBOX_APP_SECRET, DROPBOX_REFRESH_TOKEN)\n",
        "        if dbx is None:\n",
        "            return\n",
        "        \n",
        "        # Upload the file to Dropbox\n",
        "        dropbox_path = f\"/llama3-groq-tool-use-8b/{model_name}.zip\"\n",
        "        upload_to_dropbox(dbx, zip_file, dropbox_path)\n",
        "\n",
        "upload_button.on_click(on_upload_click)\n",
        "display(upload_button)\n",
        "display(output_area)\n",
        "print(\"Click the button above to upload the model to your Dropbox account.\")\n",
        "print(\"Note: This may take a while depending on your internet connection and the size of the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup-section"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "Optionally, you can run this cell to clean up temporary files and free up disk space after downloading the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup-code"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def cleanup():\n",
        "    # Check if we've already downloaded the zip file\n",
        "    if not os.path.exists(zip_file):\n",
        "        print(\"\\u26a0\ufe0f No zip file found. Make sure you've downloaded the model first.\")\n",
        "        return\n",
        "    \n",
        "    # Remove the model directory\n",
        "    if os.path.exists(model_dir):\n",
        "        print(f\"Removing model directory {model_dir}...\")\n",
        "        shutil.rmtree(model_dir)\n",
        "        print(f\"\u2705 Model directory removed.\")\n",
        "    \n",
        "    # Clear cache\n",
        "    print(\"Clearing cache...\")\n",
        "    !rm -rf ~/.cache/huggingface\n",
        "    print(\"\u2705 Cache cleared.\")\n",
        "    \n",
        "    # Show disk usage\n",
        "    print(\"\\nCurrent disk usage:\")\n",
        "    !df -h /content\n",
        "\n",
        "# Create a cleanup button\n",
        "cleanup_button = widgets.Button(\n",
        "    description=\"Clean Up Files\",\n",
        "    button_style=\"warning\",\n",
        "    icon=\"trash\"\n",
        ")\n",
        "\n",
        "def on_cleanup_click(b):\n",
        "    cleanup()\n",
        "\n",
        "cleanup_button.on_click(on_cleanup_click)\n",
        "display(cleanup_button)\n",
        "print(\"Click the button above to clean up temporary files and free up disk space.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup-section"
      },
      "source": [
        "## Optional: Clean Up\n",
        "\n",
        "If you want to free up space after downloading, you can run this cell to remove the temporary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup-code"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run this cell if you want to remove the temporary files\n",
        "# import shutil\n",
        "# shutil.rmtree(temp_dir)\n",
        "# print(f\"\u2705 Removed temporary model files at {temp_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}