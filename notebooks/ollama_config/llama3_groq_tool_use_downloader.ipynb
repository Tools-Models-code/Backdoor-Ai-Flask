{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Backdoor AI - Ollama Llama3 Groq Tool Use Model Downloader\n",
        "\n",
        "This notebook downloads the llama3-groq-tool-use model using Ollama and zips it for easy download. This notebook is designed to work with Google Colab.\n",
        "\n",
        "## How it works\n",
        "\n",
        "1. Check if we're running in Google Colab\n",
        "2. Optimize the Colab environment for large models\n",
        "3. Install Ollama and necessary dependencies\n",
        "4. Download the llama3-groq-tool-use model using Ollama\n",
        "5. Zip the model files for easy download\n",
        "6. Provide a download link for the zipped model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-check"
      },
      "source": [
        "## Check if running in Google Colab\n",
        "\n",
        "First, let's make sure we're running in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-colab"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if not IN_COLAB:\n",
        "    print(\"\u26a0\ufe0f This notebook is designed to run in Google Colab. Some features may not work as expected.\")\n",
        "else:\n",
        "    print(\"\u2705 Running in Google Colab!\")\n",
        "    \n",
        "    # Import Google Colab specific modules\n",
        "    from google.colab import drive, files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-deps"
      },
      "source": [
        "## Memory Optimization for Large Models\n",
        "\n",
        "First, let's clear up disk space and optimize memory to ensure we have enough resources for large models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-dependencies"
      },
      "outputs": [],
      "source": [
        "# Memory optimization functions\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import gc\n",
        "import time\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Install required packages first\n",
        "!pip install -q psutil tqdm\n",
        "import psutil\n",
        "\n",
        "def clear_disk_space():\n",
        "    \"\"\"Clean up disk space by removing unnecessary files.\"\"\"\n",
        "    print(\"\ud83e\uddf9 Cleaning up disk space...\")\n",
        "    \n",
        "    # Clean apt cache\n",
        "    subprocess.run(\"apt-get clean\", shell=True)\n",
        "    \n",
        "    # Remove unnecessary packages\n",
        "    subprocess.run(\"apt-get -y autoremove\", shell=True)\n",
        "    \n",
        "    # Clean pip cache\n",
        "    subprocess.run(\"rm -rf ~/.cache/pip\", shell=True)\n",
        "    \n",
        "    # Remove temporary files\n",
        "    temp_dirs = ['/tmp', '/var/tmp']\n",
        "    for temp_dir in temp_dirs:\n",
        "        if os.path.exists(temp_dir):\n",
        "            try:\n",
        "                for item in os.listdir(temp_dir):\n",
        "                    item_path = os.path.join(temp_dir, item)\n",
        "                    # Skip our ollama directories\n",
        "                    if item.startswith('ollama') or item.startswith('backdoor'):\n",
        "                        continue\n",
        "                    \n",
        "                    try:\n",
        "                        if os.path.isdir(item_path):\n",
        "                            shutil.rmtree(item_path)\n",
        "                        else:\n",
        "                            os.remove(item_path)\n",
        "                    except Exception as e:\n",
        "                        pass  # Skip files that can't be removed\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not clean {temp_dir}: {e}\")\n",
        "    \n",
        "    # Remove unused Docker images/containers if Docker is installed\n",
        "    try:\n",
        "        subprocess.run(\"docker system prune -af\", shell=True, stderr=subprocess.DEVNULL)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"\u2705 Disk cleanup complete!\")\n",
        "    \n",
        "    # Show disk space\n",
        "    show_disk_usage()\n",
        "\n",
        "def show_disk_usage():\n",
        "    \"\"\"Show current disk usage.\"\"\"\n",
        "    try:\n",
        "        df_output = subprocess.check_output(\"df -h /\", shell=True, text=True)\n",
        "        print(\"\\n\ud83d\udcca Disk Space Available:\")\n",
        "        for line in df_output.split('\\n'):\n",
        "            print(line)\n",
        "    except:\n",
        "        print(\"Could not retrieve disk usage information\")\n",
        "\n",
        "def show_memory_usage():\n",
        "    \"\"\"Show current memory usage.\"\"\"\n",
        "    try:\n",
        "        memory = psutil.virtual_memory()\n",
        "        total_gb = memory.total / (1024 ** 3)\n",
        "        available_gb = memory.available / (1024 ** 3)\n",
        "        used_gb = memory.used / (1024 ** 3)\n",
        "        percent = memory.percent\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcca Memory Usage:\")\n",
        "        print(f\"Total Memory: {total_gb:.2f} GB\")\n",
        "        print(f\"Available: {available_gb:.2f} GB\")\n",
        "        print(f\"Used: {used_gb:.2f} GB ({percent}%)\")\n",
        "    except:\n",
        "        print(\"Could not retrieve memory usage information\")\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear Python memory.\"\"\"\n",
        "    gc.collect()\n",
        "    torch_available = False\n",
        "    \n",
        "    try:\n",
        "        import torch\n",
        "        torch_available = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    if torch_available:\n",
        "        try:\n",
        "            import torch\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"\u2705 PyTorch CUDA cache cleared\")\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    print(\"\u2705 Python memory cleared\")\n",
        "\n",
        "# Run optimization process\n",
        "print(\"\ud83d\ude80 Optimizing environment for large language models...\")\n",
        "clear_disk_space()\n",
        "clear_memory()\n",
        "\n",
        "# Set environment variables for improved performance\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "\n",
        "# Show current resource usage\n",
        "show_memory_usage()\n",
        "show_disk_usage()\n",
        "\n",
        "print(\"\\n\u2705 Optimization complete! Ready to continue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-model-header"
      },
      "source": [
        "## Install Ollama\n",
        "\n",
        "Now let's install Ollama and other required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-model"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "# Install GPU detection tools first\n",
        "!apt-get update && apt-get install -y lspci lshw pciutils\n",
        "\n",
        "print(\"\ud83d\udcbe Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install -q requests httpx\n",
        "\n",
        "# Set up directories\n",
        "!mkdir -p /tmp/ollama/models\n",
        "\n",
        "print(\"\u2705 Ollama installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zip-model-header"
      },
      "source": [
        "## Start Ollama Server\n",
        "\n",
        "Now we'll start the Ollama server in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zip-model"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Start Ollama server in background\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Wait for Ollama to start\n",
        "print(\"Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if Ollama is running\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/version\", timeout=30)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"\u2705 Ollama is running! Version: {response.json().get('version')}\")\n",
        "    else:\n",
        "        print(f\"\u274c Ollama returned unexpected status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Failed to connect to Ollama: {e}\")\n",
        "    print(\"Trying to start again...\")\n",
        "    # Kill the previous process if it exists\n",
        "    if ollama_process:\n",
        "        ollama_process.terminate()\n",
        "        time.sleep(2)\n",
        "    # Try starting again\n",
        "    !ollama serve &\n",
        "    time.sleep(5)\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/version\", timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"\u2705 Second attempt succeeded! Ollama is running. Version: {response.json().get('version')}\")\n",
        "    except:\n",
        "        print(\"\u274c Failed to start Ollama after multiple attempts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-zip-header"
      },
      "source": [
        "## Download the Llama3 Groq Tool Use Model\n",
        "\n",
        "Now let's download the llama3-groq-tool-use model using Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-zip"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Set the model ID for llama3-groq-tool-use\n",
        "model_id = \"llama3-groq-tool-use:8b\"\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Function to download model\n",
        "def download_model():\n",
        "    with output_area:\n",
        "        print(f\"\ud83d\ude80 Downloading llama3-groq-tool-use model: {model_id}\")\n",
        "        print(\"This may take a while depending on your internet connection...\")\n",
        "        print(\"You'll see progress below. Please don't interrupt the process.\")\n",
        "        \n",
        "        # Run ollama pull command\n",
        "        process = subprocess.Popen(\n",
        "            [\"ollama\", \"pull\", model_id],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "        \n",
        "        # Show live output\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                # Add sleep to prevent CPU overload\n",
        "                time.sleep(0.1)\n",
        "                break\n",
        "            if output:\n",
        "                print(output.strip())\n",
        "        \n",
        "        return_code = process.poll()\n",
        "        if return_code == 0:\n",
        "            print(f\"\u2705 Model {model_id} downloaded successfully!\")\n",
        "            # List available models\n",
        "            print(\"\\n\ud83d\udccb Available models:\")\n",
        "            !ollama list\n",
        "        else:\n",
        "            print(f\"\u274c Failed to download model {model_id}. Return code: {return_code}\")\n",
        "\n",
        "# Download the model\n",
        "download_model()\n",
        "display(output_area)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup-header"
      },
      "source": [
        "## Zip the Ollama Model Files\n",
        "\n",
        "Now let's zip the model files for easy download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# Define the model name and paths\n",
        "model_name = \"llama3-groq-tool-use-8b\"\n",
        "ollama_model_dir = \"/root/.ollama/models\"\n",
        "zip_dir = \"/content\"\n",
        "zip_file = f\"{zip_dir}/{model_name}.zip\"\n",
        "\n",
        "# Create a temporary directory to store the model files\n",
        "temp_dir = f\"{zip_dir}/{model_name}\"\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Copying model files to {temp_dir}...\")\n",
        "\n",
        "# Find all files related to the model\n",
        "model_files = []\n",
        "for root, dirs, files in os.walk(ollama_model_dir):\n",
        "    for file in files:\n",
        "        if \"llama3-groq-tool-use\" in file.lower():\n",
        "            model_files.append(os.path.join(root, file))\n",
        "\n",
        "# Copy model files to the temporary directory\n",
        "for file_path in model_files:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    dest_path = os.path.join(temp_dir, file_name)\n",
        "    shutil.copy2(file_path, dest_path)\n",
        "    print(f\"  - Copied {file_name}\")\n",
        "\n",
        "# Also copy the model manifest file if it exists\n",
        "manifest_file = os.path.join(ollama_model_dir, \"manifests\", \"registry.ollama.ai\", \"library\", \"llama3-groq-tool-use\")\n",
        "if os.path.exists(manifest_file):\n",
        "    manifest_dir = os.path.join(temp_dir, \"manifest\")\n",
        "    os.makedirs(manifest_dir, exist_ok=True)\n",
        "    shutil.copy2(manifest_file, os.path.join(manifest_dir, \"manifest\"))\n",
        "    print(f\"  - Copied model manifest\")\n",
        "\n",
        "# Zip the model directory\n",
        "print(f\"\\nZipping model files to {zip_file}...\")\n",
        "shutil.make_archive(f\"{zip_dir}/{model_name}\", 'zip', zip_dir, model_name)\n",
        "\n",
        "# Get the size of the zip file\n",
        "if os.path.exists(zip_file):\n",
        "    zip_size = os.path.getsize(zip_file) / (1024 * 1024 * 1024)  # Convert to GB\n",
        "    print(f\"\u2705 Model zipped successfully to {zip_file}\")\n",
        "    print(f\"Zip file size: {zip_size:.2f} GB\")\n",
        "else:\n",
        "    print(f\"\u274c Failed to create zip file at {zip_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-section"
      },
      "source": [
        "## Download the Zipped Model\n",
        "\n",
        "Now you can download the zipped model to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-code"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    print(\"Preparing download...\")\n",
        "    from google.colab import files\n",
        "    files.download(zip_file)\n",
        "    print(\"\u2705 Download initiated. If it doesn't start automatically, check your browser's download manager.\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f Not running in Colab. The zipped model is available at {zip_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup-section"
      },
      "source": [
        "## Optional: Clean Up\n",
        "\n",
        "If you want to free up space after downloading, you can run this cell to remove the temporary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup-code"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run this cell if you want to remove the temporary files\n",
        "# import shutil\n",
        "# shutil.rmtree(temp_dir)\n",
        "# print(f\"\u2705 Removed temporary model files at {temp_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}