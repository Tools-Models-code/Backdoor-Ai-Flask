{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Llama3 Groq Tool Use Model Downloader\n",
        "\n",
        "This notebook downloads the llama3-groq-tool-use model and zips it for easy download. This notebook is designed to work with Google Colab.\n",
        "\n",
        "## How it works\n",
        "\n",
        "1. Check if we're running in Google Colab\n",
        "2. Install necessary dependencies\n",
        "3. Download the llama3-groq-tool-use model\n",
        "4. Zip the model for easy download\n",
        "5. Provide a download link for the zipped model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-check"
      },
      "source": [
        "## Check if running in Google Colab\n",
        "\n",
        "First, let's make sure we're running in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-colab"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if not IN_COLAB:\n",
        "    print(\"\u26a0\ufe0f This notebook is designed to run in Google Colab. Some features may not work as expected.\")\n",
        "else:\n",
        "    print(\"\u2705 Running in Google Colab!\")\n",
        "    \n",
        "    # Import Google Colab specific modules\n",
        "    from google.colab import drive, files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-deps"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Let's install the necessary dependencies for downloading and managing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-dependencies"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-model-header"
      },
      "source": [
        "## Download the Llama3 Groq Tool Use Model\n",
        "\n",
        "Now we'll download the model from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-model"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Define the model name and download directory\n",
        "model_name = \"llama3-groq-tool-use\"\n",
        "model_id = \"groq/llama3-8b-tool-use-preview\"\n",
        "download_dir = f\"/content/{model_name}\"\n",
        "\n",
        "print(f\"Downloading {model_id} to {download_dir}...\")\n",
        "\n",
        "# Create the download directory if it doesn't exist\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# Download the model\n",
        "snapshot_download(\n",
        "    repo_id=model_id,\n",
        "    local_dir=download_dir,\n",
        "    local_dir_use_symlinks=False,  # Don't use symlinks for better compatibility\n",
        "    tqdm_class=tqdm\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Model downloaded successfully to {download_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zip-model-header"
      },
      "source": [
        "## Zip the Model for Download\n",
        "\n",
        "Now let's zip the model to make it easy to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zip-model"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Define the zip file name\n",
        "zip_file = f\"/content/{model_name}.zip\"\n",
        "\n",
        "print(f\"Zipping model to {zip_file}...\")\n",
        "\n",
        "# Zip the model directory\n",
        "shutil.make_archive(f\"/content/{model_name}\", 'zip', '/content', model_name)\n",
        "\n",
        "print(f\"\u2705 Model zipped successfully to {zip_file}\")\n",
        "\n",
        "# Get the size of the zip file\n",
        "zip_size = os.path.getsize(zip_file) / (1024 * 1024 * 1024)  # Convert to GB\n",
        "print(f\"Zip file size: {zip_size:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-zip-header"
      },
      "source": [
        "## Download the Zipped Model\n",
        "\n",
        "Now you can download the zipped model to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-zip"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    print(\"Preparing download...\")\n",
        "    files.download(zip_file)\n",
        "    print(\"\u2705 Download initiated. If it doesn't start automatically, check your browser's download manager.\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f Not running in Colab. The zipped model is available at {zip_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup-header"
      },
      "source": [
        "## Optional: Clean Up\n",
        "\n",
        "If you want to free up space after downloading, you can run this cell to remove the original model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run this cell if you want to remove the original model files\n",
        "# import shutil\n",
        "# shutil.rmtree(download_dir)\n",
        "# print(f\"\u2705 Removed original model files at {download_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}