[
  {
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "ollama-header"
        },
        "source": [
          "# Backdoor AI - Ollama on Google Colab\n",
          "\n",
          "This notebook helps you run Ollama on Google Colab to use with your Backdoor AI application. You can install and run Ollama models (including Llama4) directly in Colab, then connect your Backdoor AI app to it.\n",
          "\n",
          "## How it works\n",
          "\n",
          "1. This notebook will first optimize your Colab environment for large models\n",
          "2. We'll install Ollama on this Colab instance\n",
          "3. You'll select and download models (Llama4 or others)\n",
          "4. We'll set up Cloudflared to create a secure tunnel to your Ollama instance\n",
          "5. You'll get a URL to use in your Backdoor AI settings\n",
          "\n",
          "Let's get started!"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "memory-optimization"
        },
        "source": [
          "## 1. Memory Optimization for Large Models\n",
          "\n",
          "First, let's clear up disk space and optimize memory to ensure we have enough resources for large models."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "memory-optimization-code"
        },
        "outputs": [],
        "source": [
          "# Memory optimization functions\n",
          "import os\n",
          "import shutil\n",
          "import subprocess\n",
          "import gc\n",
          "import time\n",
          "from IPython.display import display, HTML, clear_output\n",
          "\n",
          "# Install required packages first\n",
          "!pip install -q psutil\n",
          "import psutil\n",
          "\n",
          "def clear_disk_space():\n",
          "    \"\"\"Clean up disk space by removing unnecessary files.\"\"\"\n",
          "    print(\"\ud83e\uddf9 Cleaning up disk space...\")\n",
          "    \n",
          "    # Clean apt cache\n",
          "    subprocess.run(\"apt-get clean\", shell=True)\n",
          "    \n",
          "    # Remove unnecessary packages\n",
          "    subprocess.run(\"apt-get -y autoremove\", shell=True)\n",
          "    \n",
          "    # Clean pip cache\n",
          "    subprocess.run(\"rm -rf ~/.cache/pip\", shell=True)\n",
          "    \n",
          "    # Remove temporary files\n",
          "    temp_dirs = ['/tmp', '/var/tmp']\n",
          "    for temp_dir in temp_dirs:\n",
          "        if os.path.exists(temp_dir):\n",
          "            try:\n",
          "                for item in os.listdir(temp_dir):\n",
          "                    item_path = os.path.join(temp_dir, item)\n",
          "                    # Skip our ollama directories\n",
          "                    if item.startswith('ollama') or item.startswith('backdoor'):\n",
          "                        continue\n",
          "                    \n",
          "                    try:\n",
          "                        if os.path.isdir(item_path):\n",
          "                            shutil.rmtree(item_path)\n",
          "                        else:\n",
          "                            os.remove(item_path)\n",
          "                    except Exception as e:\n",
          "                        pass  # Skip files that can't be removed\n",
          "            except Exception as e:\n",
          "                print(f\"Warning: Could not clean {temp_dir}: {e}\")\n",
          "    \n",
          "    # Remove unused Docker images/containers if Docker is installed\n",
          "    try:\n",
          "        subprocess.run(\"docker system prune -af\", shell=True, stderr=subprocess.DEVNULL)\n",
          "    except:\n",
          "        pass\n",
          "    \n",
          "    print(\"\u2705 Disk cleanup complete!\")\n",
          "    \n",
          "    # Show disk space\n",
          "    show_disk_usage()\n",
          "\n",
          "def show_disk_usage():\n",
          "    \"\"\"Show current disk usage.\"\"\"\n",
          "    try:\n",
          "        df_output = subprocess.check_output(\"df -h /\", shell=True, text=True)\n",
          "        print(\"\\n\ud83d\udcca Disk Space Available:\")\n",
          "        for line in df_output.split('\\n'):\n",
          "            print(line)\n",
          "    except:\n",
          "        print(\"Could not retrieve disk usage information\")\n",
          "\n",
          "def show_memory_usage():\n",
          "    \"\"\"Show current memory usage.\"\"\"\n",
          "    try:\n",
          "        memory = psutil.virtual_memory()\n",
          "        total_gb = memory.total / (1024 ** 3)\n",
          "        available_gb = memory.available / (1024 ** 3)\n",
          "        used_gb = memory.used / (1024 ** 3)\n",
          "        percent = memory.percent\n",
          "        \n",
          "        print(f\"\\n\ud83d\udcca Memory Usage:\")\n",
          "        print(f\"Total Memory: {total_gb:.2f} GB\")\n",
          "        print(f\"Available: {available_gb:.2f} GB\")\n",
          "        print(f\"Used: {used_gb:.2f} GB ({percent}%)\")\n",
          "    except:\n",
          "        print(\"Could not retrieve memory usage information\")\n",
          "\n",
          "def clear_memory():\n",
          "    \"\"\"Clear Python memory.\"\"\"\n",
          "    gc.collect()\n",
          "    torch_available = False\n",
          "    \n",
          "    try:\n",
          "        import torch\n",
          "        torch_available = True\n",
          "    except ImportError:\n",
          "        pass\n",
          "    \n",
          "    if torch_available:\n",
          "        try:\n",
          "            import torch\n",
          "            torch.cuda.empty_cache()\n",
          "            print(\"\u2705 PyTorch CUDA cache cleared\")\n",
          "        except:\n",
          "            pass\n",
          "    \n",
          "    print(\"\u2705 Python memory cleared\")\n",
          "\n",
          "def clean_model_files(keep_models=None):\n",
          "    \"\"\"Clean up model files to free space, optionally keeping specified models.\"\"\"\n",
          "    if keep_models is None:\n",
          "        keep_models = []\n",
          "    \n",
          "    print(f\"\ud83e\uddf9 Cleaning model files (keeping: {', '.join(keep_models) if keep_models else 'none'})...\")\n",
          "    \n",
          "    # Clean Ollama model files (except the ones specified to keep)\n",
          "    ollama_dirs = ['/root/.ollama', '/tmp/ollama']\n",
          "    \n",
          "    for ollama_dir in ollama_dirs:\n",
          "        if os.path.exists(ollama_dir):\n",
          "            models_path = os.path.join(ollama_dir, 'models')\n",
          "            if os.path.exists(models_path):\n",
          "                for model_file in os.listdir(models_path):\n",
          "                    should_keep = False\n",
          "                    for keep_model in keep_models:\n",
          "                        if keep_model in model_file:\n",
          "                            should_keep = True\n",
          "                            break\n",
          "                    \n",
          "                    if not should_keep:\n",
          "                        try:\n",
          "                            model_path = os.path.join(models_path, model_file)\n",
          "                            if os.path.isdir(model_path):\n",
          "                                shutil.rmtree(model_path)\n",
          "                            else:\n",
          "                                os.remove(model_path)\n",
          "                            print(f\"  - Removed: {model_file}\")\n",
          "                        except Exception as e:\n",
          "                            print(f\"  - Could not remove {model_file}: {e}\")\n",
          "    \n",
          "    print(\"\u2705 Model cleanup complete!\")\n",
          "    show_disk_usage()\n",
          "\n",
          "def monitor_download_progress(model_name):\n",
          "    \"\"\"Monitor the download progress of a model.\"\"\"\n",
          "    last_size = 0\n",
          "    download_dir = '/root/.ollama/models'\n",
          "    \n",
          "    print(f\"\ud83d\udd04 Monitoring download progress for {model_name}\")\n",
          "    \n",
          "    try:\n",
          "        while True:\n",
          "            if not os.path.exists(download_dir):\n",
          "                time.sleep(1)\n",
          "                continue\n",
          "                \n",
          "            total_size = 0\n",
          "            for root, dirs, files in os.walk(download_dir):\n",
          "                for file in files:\n",
          "                    if model_name.lower() in file.lower():\n",
          "                        try:\n",
          "                            file_path = os.path.join(root, file)\n",
          "                            total_size += os.path.getsize(file_path)\n",
          "                        except:\n",
          "                            pass\n",
          "            \n",
          "            if total_size > last_size:\n",
          "                clear_output(wait=True)\n",
          "                print(f\"Downloading {model_name}...\")\n",
          "                print(f\"Downloaded: {total_size / (1024**3):.2f} GB\")\n",
          "                last_size = total_size\n",
          "            \n",
          "            time.sleep(2)\n",
          "    except KeyboardInterrupt:\n",
          "        print(\"Download monitoring stopped\")\n",
          "\n",
          "# Run optimization process\n",
          "print(\"\ud83d\ude80 Optimizing environment for large language models...\")\n",
          "clear_disk_space()\n",
          "clear_memory()\n",
          "\n",
          "# Set environment variables for improved performance\n",
          "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
          "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
          "\n",
          "# Show current resource usage\n",
          "show_memory_usage()\n",
          "show_disk_usage()\n",
          "\n",
          "print(\"\\n\u2705 Optimization complete! Ready to continue.\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "requirements-section"
        },
        "source": [
          "## 2. Set up environment\n",
          "\n",
          "Now let's install the required packages. We'll need Ollama and Cloudflared for tunneling."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "install-requirements"
        },
        "outputs": [],
        "source": [
          "# Install Ollama\n",
          "# Install GPU detection tools first\n",
          "!apt-get update && apt-get install -y lspci lshw pciutils\n",
          "\n",
          "!curl -fsSL https://ollama.com/install.sh | sh\n",
          "\n",
          "# Install cloudflared for tunneling\n",
          "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
          "!dpkg -i cloudflared-linux-amd64.deb\n",
          "\n",
          "# Install other dependencies\n",
          "!pip install -q requests pyngrok httpx\n",
          "\n",
          "# Set up directories\n",
          "!mkdir -p /tmp/ollama/models"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "start-ollama"
        },
        "source": [
          "## 2. Start Ollama server\n",
          "\n",
          "Now we'll start the Ollama server in the background."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "start-ollama-code"
        },
        "outputs": [],
        "source": [
          "import subprocess\n",
          "import time\n",
          "import requests\n",
          "import json\n",
          "from IPython.display import clear_output\n",
          "\n",
          "# Start Ollama server in background\n",
          "ollama_process = subprocess.Popen(\n",
          "    [\"ollama\", \"serve\"],\n",
          "    stdout=subprocess.PIPE,\n",
          "    stderr=subprocess.PIPE,\n",
          "    text=True\n",
          ")\n",
          "\n",
          "# Wait for Ollama to start\n",
          "print(\"Starting Ollama server...\")\n",
          "time.sleep(5)\n",
          "\n",
          "# Check if Ollama is running\n",
          "try:\n",
          "    response = requests.get(\"http://localhost:11434/api/version\", timeout=30)\n",
          "    if response.status_code == 200:\n",
          "        print(f\"\u2705 Ollama is running! Version: {response.json().get('version')}\")\n",
          "    else:\n",
          "        print(f\"\u274c Ollama returned unexpected status: {response.status_code}\")\n",
          "except Exception as e:\n",
          "    print(f\"\u274c Failed to connect to Ollama: {e}\")\n",
          "    print(\"Trying to start again...\")\n",
          "    # Kill the previous process if it exists\n",
          "    if ollama_process:\n",
          "        ollama_process.terminate()\n",
          "        time.sleep(2)\n",
          "    # Try starting again\n",
          "    !ollama serve &\n",
          "    time.sleep(5)\n",
          "    try:\n",
          "        response = requests.get(\"http://localhost:11434/api/version\", timeout=30)\n",
          "        if response.status_code == 200:\n",
          "            print(f\"\u2705 Second attempt succeeded! Ollama is running. Version: {response.json().get('version')}\")\n",
          "    except:\n",
          "        print(\"\u274c Failed to start Ollama after multiple attempts.\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "model-section"
        },
        "source": [
          "## 3. Choose and download a model\n",
          "\n",
          "Now, let's download a model. We recommend Llama4 models for best performance or compatibility."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "model-selection"
        },
        "outputs": [],
        "source": [
          "import ipywidgets as widgets\n",
          "from IPython.display import display, HTML, clear_output\n",
          "import subprocess\n",
          "import time\n",
          "\n",
          "# Set the model ID for llama3-groq-tool-use\n",
          "model_id = \"llama3-groq-tool-use:8b\"\n",
          "output_area = widgets.Output()\n",
          "\n",
          "# Function to download model\n",
          "def download_model():\n",
          "    with output_area:\n",
          "        print(f\"\ud83d\ude80 Automatically downloading llama3-groq-tool-use model: {model_id}\")\n",
          "        print(\"This may take a while depending on your internet connection...\")\n",
          "        print(\"You'll see progress below. Please don't interrupt the process.\")\n",
          "        \n",
          "        # Run ollama pull command\n",
          "        process = subprocess.Popen(\n",
          "            [\"ollama\", \"pull\", model_id],\n",
          "            stdout=subprocess.PIPE,\n",
          "            stderr=subprocess.STDOUT,\n",
          "            text=True\n",
          "        )\n",
          "        \n",
          "        # Show live output\n",
          "        while True:\n",
          "            output = process.stdout.readline()\n",
          "            if output == ''' and process.poll() is not None:\n",
          "                # Add sleep to prevent CPU overload\n",
          "                time.sleep(0.1)\n",
          "                break\n",
          "            if output:\n",
          "                print(output.strip())\n",
          "        \n",
          "        return_code = process.poll()\n",
          "        if return_code == 0:\n",
          "            print(f\"\u2705 Model {model_id} downloaded successfully!\")\n",
          "            # List available models\n",
          "            print(\"\\n\ud83d\udccb Available models:\")\n",
          "            !ollama list\n",
          "        else:\n",
          "            print(f\"\u274c Failed to download model {model_id}. Return code: {return_code}\")\n",
          "\n",
          "# Display header and start download\n",
          "display(widgets.HTML(\"<h3>Downloading llama3-groq-tool-use model:</h3>\"))\n",
          "display(output_area)\n",
          "\n",
          "# Start the download automatically\n",
          "download_model()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "test-model"
        },
        "source": [
          "## 4. Test the model\n",
          "\n",
          "Let's make sure the model works by asking it a simple question."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "test-model-code"
        },
        "outputs": [],
        "source": [
          "import ipywidgets as widgets\n",
          "import requests\n",
          "import json\n",
          "import time\n",
          "from IPython.display import display, HTML\n",
          "\n",
          "# Function to test a model\n",
          "def test_model(model_id, prompt=\"Hi, I'm testing if you're working properly. Please give a brief greeting.\", timeout=60):\n",
          "    import requests\n",
          "    import time\n",
          "    import json\n",
          "    \n",
          "    url = \"http://localhost:11434/api/chat\"\n",
          "    payload = {\n",
          "        \"model\": model_id,\n",
          "        \"messages\": [\n",
          "            {\"role\": \"user\", \"content\": prompt}\n",
          "        ],\n",
          "        \"stream\": False\n",
          "    }\n",
          "    \n",
          "    print(f\"Sending request to Ollama API at {url}...\")\n",
          "    print(f\"Request payload: {json.dumps(payload, indent=2)}\")\n",
          "    \n",
          "    start_time = time.time()\n",
          "    \n",
          "    try:\n",
          "        # Check if Ollama server is running first\n",
          "        try:\n",
          "            health_check = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
          "            print(f\"Ollama server is running. Status code: {health_check.status_code}\")\n",
          "        except Exception as e:\n",
          "            print(f\"Warning: Could not connect to Ollama server: {e}\")\n",
          "            print(\"Will still try to send the request...\")\n",
          "        \n",
          "        # Add timeout to prevent hanging\n",
          "        print(f\"Setting request timeout to {timeout} seconds\")\n",
          "        response = requests.post(url, json=payload, timeout=timeout)\n",
          "        elapsed = time.time() - start_time\n",
          "        print(f\"Received response with status code: {response.status_code} in {elapsed:.2f} seconds\")\n",
          "        \n",
          "        if response.status_code == 200:\n",
          "            result = response.json()\n",
          "            return {\n",
          "                \"success\": True,\n",
          "                \"response\": result.get(\"message\", {}).get(\"content\", \"No content returned\"),\n",
          "                \"elapsed_time\": elapsed\n",
          "            }\n",
          "        else:\n",
          "            print(f\"Error response: {response.text[:500]}\")\n",
          "            return {\n",
          "                \"success\": False,\n",
          "                \"error\": f\"Server returned status code {response.status_code}: {response.text[:500]}\",\n",
          "                \"elapsed_time\": elapsed\n",
          "            }\n",
          "    except requests.exceptions.Timeout:\n",
          "        elapsed = time.time() - start_time\n",
          "        print(f\"Request timed out after {elapsed:.2f} seconds\")\n",
          "        return {\n",
          "            \"success\": False,\n",
          "            \"error\": f\"Request timed out after {elapsed:.2f} seconds. The model might be too large for available memory or still loading.\",\n",
          "            \"elapsed_time\": elapsed\n",
          "        }\n",
          "    except Exception as e:\n",
          "        elapsed = time.time() - start_time\n",
          "        print(f\"Exception during request: {str(e)}\")\n",
          "        return {\n",
          "            \"success\": False,\n",
          "            \"error\": str(e),\n",
          "            \"elapsed_time\": elapsed\n",
          "        }\n",
          "def on_test_button_click(b):\n",
          "    with test_output:\n",
          "        clear_output()\n",
          "        if not test_model_dropdown.value:\n",
          "            print(\"\u26a0\ufe0f Please select a model to test!\")\n",
          "            return\n",
          "        \n",
          "        print(f\"\ud83d\udd0d Testing model '{test_model_dropdown.value}' with prompt: \\n{test_prompt.value}\\n\")\n",
          "        print(\"Waiting for response...\")\n",
          "        \n",
          "        result = test_model(test_model_dropdown.value, test_prompt.value, timeout=60)\n",
          "        \n",
          "        if result[\"success\"]:\n",
          "            print(\"\\n\u2705 Model responded successfully!\\n\")\n",
          "            print(\"Response:\")\n",
          "            print(\"-----------------------------------\")\n",
          "            print(result[\"response\"])\n",
          "            print(\"-----------------------------------\")\n",
          "        else:\n",
          "            print(f\"\\n\u274c Error testing model: {result['error']}\")\n",
          "\n",
          "# Connect events\n",
          "test_button.on_click(on_test_button_click)\n",
          "\n",
          "# Display widgets\n",
          "display(widgets.HTML(\"<h3>Test your model:</h3>\"))\n",
          "display(test_model_dropdown)\n",
          "display(test_prompt)\n",
          "display(test_button)\n",
          "display(test_output)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "setup-tunnel"
        },
        "source": [
          "## 5. Set up a tunnel to access your Ollama instance\n",
          "\n",
          "Now we'll set up a Cloudflare tunnel so your Backdoor AI application can access this Ollama instance."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "tunnel-code"
        },
        "outputs": [],
        "source": [
          "import ipywidgets as widgets\n",
          "import subprocess\n",
          "import threading\n",
          "import time\n",
          "import re\n",
          "from IPython.display import display, HTML\n",
          "\n",
          "# Function to run cloudflared tunnel in a separate thread\n",
          "def run_tunnel():\n",
          "    global tunnel_process, tunnel_url\n",
          "    tunnel_process = subprocess.Popen(\n",
          "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
          "        stdout=subprocess.PIPE,\n",
          "        stderr=subprocess.STDOUT,\n",
          "        text=True\n",
          "    )\n",
          "    \n",
          "    # Extract tunnel URL\n",
          "    tunnel_url = None\n",
          "    url_pattern = re.compile(r'https://[\\w.-]+\\.trycloudflare\\.com')\n",
          "    \n",
          "    while True:\n",
          "        line = tunnel_process.stdout.readline()\n",
          "        if not line and tunnel_process.poll() is not None:\n",
          "            break\n",
          "        \n",
          "        match = url_pattern.search(line)\n",
          "        if match and not tunnel_url:\n",
          "            tunnel_url = match.group(0)\n",
          "            print(f\"\u2705 Tunnel established at: {tunnel_url}\")\n",
          "            # Update tunnel info\n",
          "            tunnel_info.value = f\"<div style='padding: 10px; background-color: #e6ffe6; border-radius: 5px;'><b>\u2705 Your Ollama API is accessible at:</b><br><code>{tunnel_url}</code><br><br>Use this URL in your Backdoor AI settings as the Ollama API Base URL.<br>Keep this notebook running while you're using Ollama with your app!</div>\"\n",
          "\n",
          "# Create widgets for tunnel info\n",
          "tunnel_button = widgets.Button(description='Start Tunnel', button_style='info')\n",
          "tunnel_info = widgets.HTML(\"\")\n",
          "\n",
          "# Function to start tunnel\n",
          "def on_tunnel_button_click(b):\n",
          "    b.description = \"Starting...\"\n",
          "    b.disabled = True\n",
          "    tunnel_info.value = \"<div style='padding: 10px; background-color: #fff3e6; border-radius: 5px;'>\u23f3 Creating secure tunnel to Ollama... (this may take a moment)</div>\"\n",
          "    \n",
          "    # Start tunnel in separate thread\n",
          "    thread = threading.Thread(target=run_tunnel)\n",
          "    thread.daemon = True\n",
          "    thread.start()\n",
          "    \n",
          "    # Check for tunnel URL\n",
          "    attempts = 0\n",
          "    while attempts < 10 and not tunnel_url:\n",
          "        time.sleep(1)\n",
          "        attempts += 1\n",
          "    \n",
          "    if not tunnel_url:\n",
          "        tunnel_info.value = \"<div style='padding: 10px; background-color: #ffe6e6; border-radius: 5px;'>\u274c Failed to create tunnel. Check the output below for details.</div>\"\n",
          "        b.description = \"Try Again\"\n",
          "        b.disabled = False\n",
          "\n",
          "# Connect events\n",
          "tunnel_button.on_click(on_tunnel_button_click)\n",
          "\n",
          "# Initialize global variables\n",
          "tunnel_process = None\n",
          "tunnel_url = None\n",
          "\n",
          "# Display widgets\n",
          "display(widgets.HTML(\"<h3>Create a secure tunnel to your Ollama instance:</h3>\"))\n",
          "display(widgets.HTML(\"<p>This will create a public URL that you can use to connect your Backdoor AI application to this Ollama instance.</p>\"))\n",
          "display(tunnel_button)\n",
          "display(tunnel_info)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "backdoor-instructions"
        },
        "source": [
          "## 6. Connect Backdoor AI to your Ollama instance\n",
          "\n",
          "Once you have your tunnel URL, follow these steps to connect Backdoor AI to your Ollama instance:\n",
          "\n",
          "1. Go to your Backdoor AI settings page\n",
          "2. Select \"Ollama (Remote)\" as your LLM provider\n",
          "3. In the \"Ollama API URL\" field, enter the tunnel URL from above\n",
          "4. In the \"Ollama Model\" dropdown, select the model you downloaded\n",
          "5. Click \"Save Settings\"\n",
          "\n",
          "**Important Notes:**\n",
          "- Keep this notebook running as long as you're using Ollama with your app\n",
          "- The tunnel URL will change each time you restart this notebook\n",
          "- Google Colab sessions have limited runtime (a few hours for free tier)\n",
          "- Your model downloads will be lost when the Colab session ends\n",
          "\n",
          "If you want a more permanent solution, consider setting up Ollama on your own machine or a cloud server."
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "closing-notes"
        },
        "source": [
          "## Chat with your model\n",
          "\n",
          "Use this interface to chat directly with your model through the Cloudflare tunnel."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "chat-interface"
        },
        "outputs": [],
        "source": [
          "import ipywidgets as widgets\n",
          "import requests\n",
          "import json\n",
          "import time\n",
          "from IPython.display import display, HTML, clear_output\n",
          "\n",
          "# Create chat interface widgets\n",
          "chat_model_dropdown = widgets.Dropdown(\n",
          "    options=available_models,\n",
          "    description='Model:',\n",
          "    style={'description_width': 'initial'},\n",
          "    layout=widgets.Layout(width='50%')\n",
          ")\n",
          "\n",
          "chat_history = []\n",
          "chat_display = widgets.Output(layout=widgets.Layout(height='300px', overflow_y='auto', border='1px solid #ddd', padding='10px'))\n",
          "chat_input = widgets.Textarea(\n",
          "    placeholder='Type your message here...',\n",
          "    layout=widgets.Layout(width='100%', height='80px')\n",
          ")\n",
          "send_button = widgets.Button(description='Send', button_style='primary')\n",
          "clear_button = widgets.Button(description='Clear Chat', button_style='danger')\n",
          "\n",
          "# Function to send message to model\n",
          "def send_message(message, model_id, timeout=60):\n",
          "    url = \"http://localhost:11434/api/chat\"\n",
          "    payload = {\n",
          "        \"model\": model_id,\n",
          "        \"messages\": chat_history + [{\"role\": \"user\", \"content\": message}],\n",
          "        \"stream\": False\n",
          "    }\n",
          "    \n",
          "    try:\n",
          "        response = requests.post(url, json=payload, timeout=timeout)\n",
          "        if response.status_code == 200:\n",
          "            result = response.json()\n",
          "            return {\n",
          "                \"success\": True,\n",
          "                \"response\": result.get(\"message\", {}).get(\"content\", \"No content returned\")\n",
          "            }\n",
          "        else:\n",
          "            return {\n",
          "                \"success\": False,\n",
          "                \"error\": f\"Server returned status code {response.status_code}: {response.text[:500]}\"\n",
          "            }\n",
          "    except Exception as e:\n",
          "        return {\n",
          "            \"success\": False,\n",
          "            \"error\": str(e)\n",
          "        }\n",
          "\n",
          "# Handle send button click\n",
          "def on_send_button_click(b):\n",
          "    user_message = chat_input.value.strip()\n",
          "    if not user_message or not chat_model_dropdown.value:\n",
          "        return\n",
          "    \n",
          "    # Clear input\n",
          "    chat_input.value = \"\"\n",
          "    \n",
          "    # Add user message to chat history\n",
          "    chat_history.append({\"role\": \"user\", \"content\": user_message})\n",
          "    \n",
          "    # Display user message\n",
          "    with chat_display:\n",
          "        display(HTML(f\"<div style='margin-bottom: 10px;'><b>You:</b> {user_message}</div>\"))\n",
          "    \n",
          "    # Show typing indicator\n",
          "    with chat_display:\n",
          "        typing_indicator = display(HTML(\"<div id='typing'><b>Model:</b> Thinking...</div>\"))\n",
          "    \n",
          "    # Get model response\n",
          "    result = send_message(user_message, chat_model_dropdown.value)\n",
          "    \n",
          "    # Remove typing indicator and display response\n",
          "    with chat_display:\n",
          "        clear_output(wait=True)\n",
          "        for msg in chat_history:\n",
          "            if msg[\"role\"] == \"user\":\n",
          "                display(HTML(f\"<div style='margin-bottom: 10px;'><b>You:</b> {msg['content']}</div>\"))\n",
          "            else:\n",
          "                display(HTML(f\"<div style='margin-bottom: 10px;'><b>Model:</b> {msg['content']}</div>\"))\n",
          "    \n",
          "    if result[\"success\"]:\n",
          "        # Add model response to chat history\n",
          "        chat_history.append({\"role\": \"assistant\", \"content\": result[\"response\"]})\n",
          "        \n",
          "        # Display model response\n",
          "        with chat_display:\n",
          "            display(HTML(f\"<div style='margin-bottom: 10px;'><b>Model:</b> {result['response']}</div>\"))\n",
          "    else:\n",
          "        # Display error\n",
          "        with chat_display:\n",
          "            display(HTML(f\"<div style='color: red; margin-bottom: 10px;'><b>Error:</b> {result['error']}</div>\"))\n",
          "\n",
          "# Handle clear button click\n",
          "def on_clear_button_click(b):\n",
          "    global chat_history\n",
          "    chat_history = []\n",
          "    with chat_display:\n",
          "        clear_output()\n",
          "        display(HTML(\"<div style='color: gray;'>Chat cleared. Start a new conversation.</div>\"))\n",
          "\n",
          "# Connect events\n",
          "send_button.on_click(on_send_button_click)\n",
          "clear_button.on_click(on_clear_button_click)\n",
          "\n",
          "# Handle Enter key in textarea\n",
          "def on_key_press(widget):\n",
          "    if widget.value.endswith('\\n'):\n",
          "        # Remove trailing newline\n",
          "        widget.value = widget.value.rstrip()\n",
          "        # Trigger send button\n",
          "        on_send_button_click(None)\n",
          "\n",
          "chat_input.observe(on_key_press, names='value')\n",
          "\n",
          "# Display widgets\n",
          "display(widgets.HTML(\"<h3>Chat with your model:</h3>\"))\n",
          "display(chat_model_dropdown)\n",
          "display(chat_display)\n",
          "display(chat_input)\n",
          "display(widgets.HBox([send_button, clear_button]))\n",
          "\n",
          "# Initialize chat display\n",
          "with chat_display:\n",
          "    display(HTML(\"<div style='color: gray;'>Select a model and start chatting!</div>\"))\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "additional-info"
        },
        "source": [
          "## Additional Information\n",
          "\n",
          "### Troubleshooting\n",
          "\n",
          "If you encounter issues:\n",
          "\n",
          "1. **Ollama not starting**: Try restarting the runtime (Runtime > Restart runtime) and run all cells again\n",
          "2. **Model download failing**: Check your internet connection and try a smaller model\n",
          "3. **Tunnel not working**: Make sure Ollama is running properly, then try starting the tunnel again\n",
          "4. **Chat not working**: Verify that your model is loaded correctly using the test function above\n",
          "\n",
          "### Keeping Colab Active\n",
          "\n",
          "Google Colab sessions will disconnect after periods of inactivity. To keep your session active:\n",
          "- Keep the browser tab open\n",
          "- Consider using browser extensions that simulate activity\n",
          "- Interact with the chat interface regularly\n",
          "\n",
          "### Shutting Down\n",
          "\n",
          "When you're done, remember to:\n",
          "1. Switch your Backdoor AI back to another provider if needed\n",
          "2. Close this notebook\n",
          "\n",
          "### For Production Use\n",
          "\n",
          "For a more reliable solution, consider:\n",
          "- Running Ollama on your own hardware or a cloud VM\n",
          "- Setting up proper authentication and security measures\n",
          "- Using persistent storage for model files"
        ]
      }
    ],
    "metadata": {
      "colab": {
        "provenance": [],
        "toc_visible": true
      },
      "kernelspec": {
        "display_name": "Python 3",
        "name": "python3"
      },
      "language_info": {
        "name": "python"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 0
  },
  {
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "ollama-header"
        },
        "source": [
          "# Backdoor AI - Ollama on Google Colab (Llama3 Groq Tool Use 8B)\n",
          "\n",
          "This notebook helps you run Ollama on Google Colab to use with your Backdoor AI application. You can install and run Llama3 Groq Tool Use 8B directly in Colab, then connect your Backdoor AI app to it.\n",
          "\n",
          "## How it works\n",
          "\n",
          "1. This notebook will first optimize your Colab environment for large models\n",
          "2. We'll install Ollama on this Colab instance\n",
          "3. We'll download the Llama3 Groq Tool Use 8B model\n",
          "4. We'll set up Cloudflared to create a secure tunnel to your Ollama instance\n",
          "5. You'll get a URL to use in your Backdoor AI settings\n",
          "\n",
          "Let's get started!"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "memory-optimization"
        },
        "source": [
          "## 1. Memory Optimization for Large Models\n",
          "\n",
          "First, let's clear up disk space and optimize memory to ensure we have enough resources for large models."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "memory-optimization-code"
        },
        "outputs": [],
        "source": [
          "# Memory optimization functions\n",
          "import os\n",
          "import shutil\n",
          "import subprocess\n",
          "import gc\n",
          "import time\n",
          "from IPython.display import display, HTML, clear_output\n",
          "\n",
          "# Install required packages first\n",
          "!pip install -q psutil\n",
          "import psutil\n",
          "\n",
          "def clear_disk_space():\n",
          "    \"\"\"Clean up disk space by removing unnecessary files.\"\"\"\n",
          "    print(\"\ud83e\uddf9 Cleaning up disk space...\")\n",
          "    \n",
          "    # Clean apt cache\n",
          "    subprocess.run(\"apt-get clean\", shell=True)\n",
          "    \n",
          "    # Remove unnecessary packages\n",
          "    subprocess.run(\"apt-get -y autoremove\", shell=True)\n",
          "    \n",
          "    # Clean pip cache\n",
          "    subprocess.run(\"rm -rf ~/.cache/pip\", shell=True)\n",
          "    \n",
          "    # Remove temporary files\n",
          "    temp_dirs = ['/tmp', '/var/tmp']\n",
          "    for temp_dir in temp_dirs:\n",
          "        if os.path.exists(temp_dir):\n",
          "            try:\n",
          "                for item in os.listdir(temp_dir):\n",
          "                    item_path = os.path.join(temp_dir, item)\n",
          "                    # Skip our ollama directories\n",
          "                    if item.startswith('ollama') or item.startswith('backdoor'):\n",
          "                        continue\n",
          "                    \n",
          "                    try:\n",
          "                        if os.path.isdir(item_path):\n",
          "                            shutil.rmtree(item_path)\n",
          "                        else:\n",
          "                            os.remove(item_path)\n",
          "                    except Exception as e:\n",
          "                        pass  # Skip files that can't be removed\n",
          "            except Exception as e:\n",
          "                print(f\"Warning: Could not clean {temp_dir}: {e}\")\n",
          "    \n",
          "    # Remove unused Docker images/containers if Docker is installed\n",
          "    try:\n",
          "        subprocess.run(\"docker system prune -af\", shell=True, stderr=subprocess.DEVNULL)\n",
          "    except:\n",
          "        pass\n",
          "    \n",
          "    print(\"\u2705 Disk cleanup complete!\")\n",
          "    \n",
          "    # Show disk space\n",
          "    show_disk_usage()\n",
          "\n",
          "def show_disk_usage():\n",
          "    \"\"\"Show current disk usage.\"\"\"\n",
          "    try:\n",
          "        df_output = subprocess.check_output(\"df -h /\", shell=True, text=True)\n",
          "        print(\"\\n\ud83d\udcca Disk Space Available:\")\n",
          "        for line in df_output.split('\\n'):\n",
          "            print(line)\n",
          "    except:\n",
          "        print(\"Could not retrieve disk usage information\")\n",
          "\n",
          "def show_memory_usage():\n",
          "    \"\"\"Show current memory usage.\"\"\"\n",
          "    try:\n",
          "        memory = psutil.virtual_memory()\n",
          "        total_gb = memory.total / (1024 ** 3)\n",
          "        available_gb = memory.available / (1024 ** 3)\n",
          "        used_gb = memory.used / (1024 ** 3)\n",
          "        percent = memory.percent\n",
          "        \n",
          "        print(f\"\\n\ud83d\udcca Memory Usage:\")\n",
          "        print(f\"Total Memory: {total_gb:.2f} GB\")\n",
          "        print(f\"Available: {available_gb:.2f} GB\")\n",
          "        print(f\"Used: {used_gb:.2f} GB ({percent}%)\")\n",
          "    except:\n",
          "        print(\"Could not retrieve memory usage information\")\n",
          "\n",
          "def clear_memory():\n",
          "    \"\"\"Clear Python memory.\"\"\"\n",
          "    gc.collect()\n",
          "    torch_available = False\n",
          "    \n",
          "    try:\n",
          "        import torch\n",
          "        torch_available = True\n",
          "    except ImportError:\n",
          "        pass\n",
          "    \n",
          "    if torch_available:\n",
          "        try:\n",
          "            import torch\n",
          "            torch.cuda.empty_cache()\n",
          "            print(\"\u2705 PyTorch CUDA cache cleared\")\n",
          "        except:\n",
          "            pass\n",
          "    \n",
          "    print(\"\u2705 Python memory cleared\")\n",
          "\n",
          "def clean_model_files(keep_models=None):\n",
          "    \"\"\"Clean up model files to free space, optionally keeping specified models.\"\"\"\n",
          "    if keep_models is None:\n",
          "        keep_models = []\n",
          "    \n",
          "    print(f\"\ud83e\uddf9 Cleaning model files (keeping: {', '.join(keep_models) if keep_models else 'none'})...\")\n",
          "    \n",
          "    # Clean Ollama model files (except the ones specified to keep)\n",
          "    ollama_dirs = ['/root/.ollama', '/tmp/ollama']\n",
          "    \n",
          "    for ollama_dir in ollama_dirs:\n",
          "        if os.path.exists(ollama_dir):\n",
          "            models_path = os.path.join(ollama_dir, 'models')\n",
          "            if os.path.exists(models_path):\n",
          "                for model_file in os.listdir(models_path):\n",
          "                    should_keep = False\n",
          "                    for keep_model in keep_models:\n",
          "                        if keep_model in model_file:\n",
          "                            should_keep = True\n",
          "                            break\n",
          "                    \n",
          "                    if not should_keep:\n",
          "                        try:\n",
          "                            model_path = os.path.join(models_path, model_file)\n",
          "                            if os.path.isdir(model_path):\n",
          "                                shutil.rmtree(model_path)\n",
          "                            else:\n",
          "                                os.remove(model_path)\n",
          "                            print(f\"  - Removed: {model_file}\")\n",
          "                        except Exception as e:\n",
          "                            print(f\"  - Could not remove {model_file}: {e}\")\n",
          "    \n",
          "    print(\"\u2705 Model cleanup complete!\")\n",
          "    show_disk_usage()\n",
          "\n",
          "def monitor_download_progress(model_name):\n",
          "    \"\"\"Monitor the download progress of a model.\"\"\"\n",
          "    last_size = 0\n",
          "    download_dir = '/root/.ollama/models'\n",
          "    \n",
          "    print(f\"\ud83d\udd04 Monitoring download progress for {model_name}\")\n",
          "    \n",
          "    try:\n",
          "        while True:\n",
          "            if not os.path.exists(download_dir):\n",
          "                time.sleep(1)\n",
          "                continue\n",
          "                \n",
          "            total_size = 0\n",
          "            for root, dirs, files in os.walk(download_dir):\n",
          "                for file in files:\n",
          "                    if model_name.lower() in file.lower():\n",
          "                        try:\n",
          "                            file_path = os.path.join(root, file)\n",
          "                            total_size += os.path.getsize(file_path)\n",
          "                        except:\n",
          "                            pass\n",
          "            \n",
          "            if total_size > last_size:\n",
          "                clear_output(wait=True)\n",
          "                print(f\"Downloading {model_name}...\")\n",
          "                print(f\"Downloaded: {total_size / (1024**3):.2f} GB\")\n",
          "                last_size = total_size\n",
          "            \n",
          "            time.sleep(2)\n",
          "    except KeyboardInterrupt:\n",
          "        print(\"Download monitoring stopped\")\n",
          "\n",
          "# Run optimization process\n",
          "print(\"\ud83d\ude80 Optimizing environment for large language models...\")\n",
          "clear_disk_space()\n",
          "clear_memory()\n",
          "\n",
          "# Set environment variables for improved performance\n",
          "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
          "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
          "\n",
          "# Show current resource usage\n",
          "show_memory_usage()\n",
          "show_disk_usage()\n",
          "\n",
          "print(\"\\n\u2705 Optimization complete! Ready to continue.\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "requirements-section"
        },
        "source": [
          "## 2. Set up environment\n",
          "\n",
          "Now let's install the required packages. We'll need Ollama and Cloudflared for tunneling."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "install-requirements"
        },
        "outputs": [],
        "source": [
          "# Install Ollama\n",
          "!curl -fsSL https://ollama.com/install.sh | sh\n",
          "\n",
          "# Install cloudflared for tunneling\n",
          "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
          "!dpkg -i cloudflared-linux-amd64.deb\n",
          "\n",
          "# Install other dependencies\n",
          "!pip install -q requests pyngrok httpx\n",
          "\n",
          "# Set up directories\n",
          "!mkdir -p /tmp/ollama/models"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "start-ollama"
        },
        "source": [
          "## 3. Start Ollama server\n",
          "\n",
          "Now we'll start the Ollama server in the background."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "start-ollama-code"
        },
        "outputs": [],
        "source": [
          "import subprocess\n",
          "import time\n",
          "import requests\n",
          "import json\n",
          "from IPython.display import clear_output\n",
          "\n",
          "# Start Ollama server in background\n",
          "ollama_process = subprocess.Popen(\n",
          "    [\"ollama\", \"serve\"],\n",
          "    stdout=subprocess.PIPE,\n",
          "    stderr=subprocess.PIPE,\n",
          "    text=True\n",
          ")\n",
          "\n",
          "# Wait for Ollama to start\n",
          "print(\"Starting Ollama server...\")\n",
          "time.sleep(5)\n",
          "\n",
          "# Check if Ollama is running\n",
          "try:\n",
          "    response = requests.get(\"http://localhost:11434/api/version\", timeout=30)\n",
          "    if response.status_code == 200:\n",
          "        print(f\"\u2705 Ollama is running! Version: {response.json().get('version')}\")\n",
          "    else:\n",
          "        print(f\"\u274c Ollama returned unexpected status: {response.status_code}\")\n",
          "except Exception as e:\n",
          "    print(f\"\u274c Failed to connect to Ollama: {e}\")\n",
          "    print(\"Trying to start again...\")\n",
          "    # Kill the previous process if it exists\n",
          "    if ollama_process:\n",
          "        ollama_process.terminate()\n",
          "        time.sleep(2)\n",
          "    # Try starting again\n",
          "    !ollama serve &\n",
          "    time.sleep(5)\n",
          "    try:\n",
          "        response = requests.get(\"http://localhost:11434/api/version\", timeout=30)\n",
          "        if response.status_code == 200:\n",
          "            print(f\"\u2705 Second attempt succeeded! Ollama is running. Version: {response.json().get('version')}\")\n",
          "    except:\n",
          "        print(\"\u274c Failed to start Ollama after multiple attempts.\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "download-llama3-groq-tool-use-8b"
        },
        "source": [
          "## 4. Download Llama3 Groq Tool Use 8B model\n",
          "\n",
          "Now, let's download the Llama3 Groq Tool Use 8B model. This is a medium-sized model (approximately 8GB) that's specifically fine-tuned for tool use and function calling."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "download-llama3-groq-tool-use-8b-code"
        },
        "outputs": [],
        "source": [
          "# Download Llama3 Groq Tool Use 8B\n",
          "print(\"\ud83d\ude80 Downloading Llama3 Groq Tool Use 8B model...\")\n",
          "print(\"This is a medium-sized model (approximately 8GB) that's specifically fine-tuned for tool use and function calling.\")\n",
          "print(\"You'll see progress below. Please don't interrupt the process.\")\n",
          "\n",
          "# Run the download command\n",
          "!ollama pull llama3-groq-tool-use:8b\n",
          "\n",
          "# Verify the model is available\n",
          "print(\"\\n\ud83d\udccb Available models:\")\n",
          "!ollama list"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "test-model"
        },
        "source": [
          "## 5. Test the Llama3 Groq Tool Use 8B model\n",
          "\n",
          "Let's make sure the model works by asking it a question relevant to its use in Backdoor AI, focusing on its tool use capabilities."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "test-llama3-groq-tool-use-8b"
        },
        "outputs": [],
        "source": [
          "import requests\n",
          "import json\n",
          "from IPython.display import display, HTML\n",
          "\n",
          "# Define a relevant prompt for Backdoor AI usage with tool use focus\n",
          "test_prompt = \"\"\"\n",
          "I'm using you with the Backdoor AI Flask application. Can you:\n",
          "1. Help me understand how to use your tool-use capabilities effectively\n",
          "2. Explain what makes you different from other models for function calling\n",
          "3. Suggest some good use cases for your tool-use abilities in a web application\n",
          "\n",
          "Please provide a brief response.\n",
          "\"\"\"\n",
          "\n",
          "# Set up the API call\n",
          "url = \"http://localhost:11434/api/chat\"\n",
          "payload = {\n",
          "    \"model\": \"llama3-groq-tool-use:8b\",\n",
          "    \"messages\": [\n",
          "        {\"role\": \"user\", \"content\": test_prompt}\n",
          "    ],\n",
          "    \"stream\": False\n",
          "}\n",
          "\n",
          "# Make the API call\n",
          "try:\n",
          "    print(\"Testing Llama3 Groq Tool Use 8B with a Backdoor AI related question...\\n\")\n",
          "    response = requests.post(url, json=payload, timeout=30)\n",
          "    if response.status_code == 200:\n",
          "        result = response.json()\n",
          "        content = result.get(\"message\", {}).get(\"content\", \"No content returned\")\n",
          "        print(\"\u2705 Model response:\\n\")\n",
          "        print(content)\n",
          "    else:\n",
          "        print(f\"\u274c Error: Server returned status {response.status_code}\")\n",
          "        print(response.text)\n",
          "except Exception as e:\n",
          "    print(f\"\u274c Error testing model: {str(e)}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "test-tool-use"
        },
        "source": [
          "## 6. Test Function Calling Capabilities\n",
          "\n",
          "Let's test the model's function calling capabilities, which is its specialty."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "test-function-calling"
        },
        "outputs": [],
        "source": [
          "import requests\n",
          "import json\n",
          "from IPython.display import display, HTML\n",
          "\n",
          "# Define a function calling test\n",
          "tools = [\n",
          "    {\n",
          "        \"type\": \"function\",\n",
          "        \"function\": {\n",
          "            \"name\": \"get_weather\",\n",
          "            \"description\": \"Get the current weather in a given location\",\n",
          "            \"parameters\": {\n",
          "                \"type\": \"object\",\n",
          "                \"properties\": {\n",
          "                    \"location\": {\n",
          "                        \"type\": \"string\",\n",
          "                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
          "                    },\n",
          "                    \"unit\": {\n",
          "                        \"type\": \"string\",\n",
          "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
          "                        \"description\": \"The temperature unit to use\"\n",
          "                    }\n",
          "                },\n",
          "                \"required\": [\"location\"]\n",
          "            }\n",
          "        }\n",
          "    }\n",
          "]\n",
          "\n",
          "# Set up the API call with tools\n",
          "url = \"http://localhost:11434/api/chat\"\n",
          "payload = {\n",
          "    \"model\": \"llama3-groq-tool-use:8b\",\n",
          "    \"messages\": [\n",
          "        {\"role\": \"user\", \"content\": \"What's the weather like in New York?\"}\n",
          "    ],\n",
          "    \"tools\": tools,\n",
          "    \"stream\": False\n",
          "}\n",
          "\n",
          "# Make the API call\n",
          "try:\n",
          "    print(\"Testing function calling capabilities...\\n\")\n",
          "    response = requests.post(url, json=payload, timeout=30)\n",
          "    if response.status_code == 200:\n",
          "        result = response.json()\n",
          "        print(\"\u2705 Model response:\\n\")\n",
          "        print(json.dumps(result, indent=2))\n",
          "        \n",
          "        # Check if the model called the function\n",
          "        if \"tool_calls\" in result.get(\"message\", {}):\n",
          "            print(\"\\n\u2705 The model successfully used function calling!\")\n",
          "        else:\n",
          "            print(\"\\n\u274c The model did not use function calling.\")\n",
          "    else:\n",
          "        print(f\"\u274c Error: Server returned status {response.status_code}\")\n",
          "        print(response.text)\n",
          "except Exception as e:\n",
          "    print(f\"\u274c Error testing function calling: {str(e)}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "setup-tunnel"
        },
        "source": [
          "## 7. Set up a tunnel to access your Ollama instance\n",
          "\n",
          "Now we'll set up a Cloudflare tunnel so your Backdoor AI application can access this Ollama instance."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "tunnel-code"
        },
        "outputs": [],
        "source": [
          "import subprocess\n",
          "import ipywidgets as widgets\n",
          "import threading\n",
          "import time\n",
          "import re\n",
          "from IPython.display import display, HTML\n",
          "\n",
          "# Function to run cloudflared tunnel in a separate thread\n",
          "def run_tunnel():\n",
          "    global tunnel_process, tunnel_url\n",
          "    tunnel_process = subprocess.Popen(\n",
          "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
          "        stdout=subprocess.PIPE,\n",
          "        stderr=subprocess.STDOUT,\n",
          "        text=True\n",
          "    )\n",
          "    \n",
          "    # Extract tunnel URL\n",
          "    tunnel_url = None\n",
          "    url_pattern = re.compile(r'https://[\\w.-]+\\.trycloudflare\\.com')\n",
          "    \n",
          "    while True:\n",
          "        line = tunnel_process.stdout.readline()\n",
          "        if not line and tunnel_process.poll() is not None:\n",
          "            break\n",
          "        \n",
          "        match = url_pattern.search(line)\n",
          "        if match and not tunnel_url:\n",
          "            tunnel_url = match.group(0)\n",
          "            print(f\"\u2705 Tunnel established at: {tunnel_url}\")\n",
          "            # Update tunnel info\n",
          "            tunnel_info.value = f\"<div style='padding: 10px; background-color: #e6ffe6; border-radius: 5px;'><b>\u2705 Your Ollama API is accessible at:</b><br><code>{tunnel_url}</code><br><br>Use this URL in your Backdoor AI settings as the Ollama API Base URL.<br>Keep this notebook running while you're using Ollama with your app!</div>\"\n",
          "\n",
          "# Create widgets for tunnel info\n",
          "tunnel_button = widgets.Button(description='Start Tunnel', button_style='info')\n",
          "tunnel_info = widgets.HTML(\"\")\n",
          "\n",
          "# Function to start tunnel\n",
          "def on_tunnel_button_click(b):\n",
          "    b.description = \"Starting...\"\n",
          "    b.disabled = True\n",
          "    tunnel_info.value = \"<div style='padding: 10px; background-color: #fff3e6; border-radius: 5px;'>\u23f3 Creating secure tunnel to Ollama... (this may take a moment)</div>\"\n",
          "    \n",
          "    # Start tunnel in separate thread\n",
          "    thread = threading.Thread(target=run_tunnel)\n",
          "    thread.daemon = True\n",
          "    thread.start()\n",
          "    \n",
          "    # Check for tunnel URL\n",
          "    attempts = 0\n",
          "    while attempts < 10 and not tunnel_url:\n",
          "        time.sleep(1)\n",
          "        attempts += 1\n",
          "    \n",
          "    if not tunnel_url:\n",
          "        tunnel_info.value = \"<div style='padding: 10px; background-color: #ffe6e6; border-radius: 5px;'>\u274c Failed to create tunnel. Check the output below for details.</div>\"\n",
          "        b.description = \"Try Again\"\n",
          "        b.disabled = False\n",
          "\n",
          "# Connect events\n",
          "tunnel_button.on_click(on_tunnel_button_click)\n",
          "\n",
          "# Initialize global variables\n",
          "tunnel_process = None\n",
          "tunnel_url = None\n",
          "\n",
          "# Display widgets\n",
          "display(widgets.HTML(\"<h3>Create a secure tunnel to your Ollama instance:</h3>\"))\n",
          "display(widgets.HTML(\"<p>This will create a public URL that you can use to connect your Backdoor AI application to this Ollama instance.</p>\"))\n",
          "display(tunnel_button)\n",
          "display(tunnel_info)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "backdoor-instructions"
        },
        "source": [
          "## 8. Connect Backdoor AI to your Ollama instance\n",
          "\n",
          "Once you have your tunnel URL, follow these steps to connect Backdoor AI to your Ollama instance:\n",
          "\n",
          "1. Go to your Backdoor AI settings page\n",
          "2. Select \"Ollama (Remote)\" as your LLM provider\n",
          "3. In the \"Ollama API URL\" field, enter the tunnel URL from above\n",
          "4. In the \"Ollama Model\" dropdown, select \"llama3-groq-tool-use:8b\"\n",
          "5. Click \"Save Settings\"\n",
          "\n",
          "**Important Notes:**\n",
          "- Keep this notebook running as long as you're using Ollama with your app\n",
          "- The tunnel URL will change each time you restart this notebook\n",
          "- Google Colab sessions have limited runtime (a few hours for free tier)\n",
          "- Your model downloads will be lost when the Colab session ends\n",
          "- This model is specifically optimized for tool use and function calling, making it ideal for applications that need to interact with external tools and APIs\n",
          "\n",
          "If you want a more permanent solution, consider setting up Ollama on your own machine or a cloud server."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {
          "id": "chat-interface"
        },
        "outputs": [],
        "source": [
          "import ipywidgets as widgets\n",
          "import requests\n",
          "import json\n",
          "import time\n",
          "from IPython.display import display, HTML, clear_output\n",
          "\n",
          "# Create chat interface widgets\n",
          "chat_model_dropdown = widgets.Dropdown(\n",
          "    options=available_models,\n",
          "    description='Model:',\n",
          "    style={'description_width': 'initial'},\n",
          "    layout=widgets.Layout(width='50%')\n",
          ")\n",
          "\n",
          "chat_history = []\n",
          "chat_display = widgets.Output(layout=widgets.Layout(height='300px', overflow_y='auto', border='1px solid #ddd', padding='10px'))\n",
          "chat_input = widgets.Textarea(\n",
          "    placeholder='Type your message here...',\n",
          "    layout=widgets.Layout(width='100%', height='80px')\n",
          ")\n",
          "send_button = widgets.Button(description='Send', button_style='primary')\n",
          "clear_button = widgets.Button(description='Clear Chat', button_style='danger')\n",
          "\n",
          "# Function to send message to model\n",
          "def send_message(message, model_id, timeout=60):\n",
          "    url = \"http://localhost:11434/api/chat\"\n",
          "    payload = {\n",
          "        \"model\": model_id,\n",
          "        \"messages\": chat_history + [{\"role\": \"user\", \"content\": message}],\n",
          "        \"stream\": False\n",
          "    }\n",
          "    \n",
          "    try:\n",
          "        response = requests.post(url, json=payload, timeout=timeout)\n",
          "        if response.status_code == 200:\n",
          "            result = response.json()\n",
          "            return {\n",
          "                \"success\": True,\n",
          "                \"response\": result.get(\"message\", {}).get(\"content\", \"No content returned\")\n",
          "            }\n",
          "        else:\n",
          "            return {\n",
          "                \"success\": False,\n",
          "                \"error\": f\"Server returned status code {response.status_code}: {response.text[:500]}\"\n",
          "            }\n",
          "    except Exception as e:\n",
          "        return {\n",
          "            \"success\": False,\n",
          "            \"error\": str(e)\n",
          "        }\n",
          "\n",
          "# Handle send button click\n",
          "def on_send_button_click(b):\n",
          "    user_message = chat_input.value.strip()\n",
          "    if not user_message or not chat_model_dropdown.value:\n",
          "        return\n",
          "    \n",
          "    # Clear input\n",
          "    chat_input.value = \"\"\n",
          "    \n",
          "    # Add user message to chat history\n",
          "    chat_history.append({\"role\": \"user\", \"content\": user_message})\n",
          "    \n",
          "    # Display user message\n",
          "    with chat_display:\n",
          "        display(HTML(f\"<div style='margin-bottom: 10px;'><b>You:</b> {user_message}</div>\"))\n",
          "    \n",
          "    # Show typing indicator\n",
          "    with chat_display:\n",
          "        typing_indicator = display(HTML(\"<div id='typing'><b>Model:</b> Thinking...</div>\"))\n",
          "    \n",
          "    # Get model response\n",
          "    result = send_message(user_message, chat_model_dropdown.value)\n",
          "    \n",
          "    # Remove typing indicator and display response\n",
          "    with chat_display:\n",
          "        clear_output(wait=True)\n",
          "        for msg in chat_history:\n",
          "            if msg[\"role\"] == \"user\":\n",
          "                display(HTML(f\"<div style='margin-bottom: 10px;'><b>You:</b> {msg['content']}</div>\"))\n",
          "            else:\n",
          "                display(HTML(f\"<div style='margin-bottom: 10px;'><b>Model:</b> {msg['content']}</div>\"))\n",
          "    \n",
          "    if result[\"success\"]:\n",
          "        # Add model response to chat history\n",
          "        chat_history.append({\"role\": \"assistant\", \"content\": result[\"response\"]})\n",
          "        \n",
          "        # Display model response\n",
          "        with chat_display:\n",
          "            display(HTML(f\"<div style='margin-bottom: 10px;'><b>Model:</b> {result['response']}</div>\"))\n",
          "    else:\n",
          "        # Display error\n",
          "        with chat_display:\n",
          "            display(HTML(f\"<div style='color: red; margin-bottom: 10px;'><b>Error:</b> {result['error']}</div>\"))\n",
          "\n",
          "# Handle clear button click\n",
          "def on_clear_button_click(b):\n",
          "    global chat_history\n",
          "    chat_history = []\n",
          "    with chat_display:\n",
          "        clear_output()\n",
          "        display(HTML(\"<div style='color: gray;'>Chat cleared. Start a new conversation.</div>\"))\n",
          "\n",
          "# Connect events\n",
          "send_button.on_click(on_send_button_click)\n",
          "clear_button.on_click(on_clear_button_click)\n",
          "\n",
          "# Handle Enter key in textarea\n",
          "def on_key_press(widget):\n",
          "    if widget.value.endswith('\\n'):\n",
          "        # Remove trailing newline\n",
          "        widget.value = widget.value.rstrip()\n",
          "        # Trigger send button\n",
          "        on_send_button_click(None)\n",
          "\n",
          "chat_input.observe(on_key_press, names='value')\n",
          "\n",
          "# Display widgets\n",
          "display(widgets.HTML(\"<h3>Chat with your model:</h3>\"))\n",
          "display(chat_model_dropdown)\n",
          "display(chat_display)\n",
          "display(chat_input)\n",
          "display(widgets.HBox([send_button, clear_button]))\n",
          "\n",
          "# Initialize chat display\n",
          "with chat_display:\n",
          "    display(HTML(\"<div style='color: gray;'>Select a model and start chatting!</div>\"))\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "additional-info"
        },
        "source": [
          "## Additional Information\n",
          "\n",
          "### Troubleshooting\n",
          "\n",
          "If you encounter issues:\n",
          "\n",
          "1. **Ollama not starting**: Try restarting the runtime (Runtime > Restart runtime) and run all cells again\n",
          "2. **Model download failing**: Check your internet connection and try a smaller model\n",
          "3. **Tunnel not working**: Make sure Ollama is running properly, then try starting the tunnel again\n",
          "4. **Chat not working**: Verify that your model is loaded correctly using the test function above\n",
          "\n",
          "### Keeping Colab Active\n",
          "\n",
          "Google Colab sessions will disconnect after periods of inactivity. To keep your session active:\n",
          "- Keep the browser tab open\n",
          "- Consider using browser extensions that simulate activity\n",
          "- Interact with the chat interface regularly\n",
          "\n",
          "### Shutting Down\n",
          "\n",
          "When you're done, remember to:\n",
          "1. Switch your Backdoor AI back to another provider if needed\n",
          "2. Close this notebook\n",
          "\n",
          "### For Production Use\n",
          "\n",
          "For a more reliable solution, consider:\n",
          "- Running Ollama on your own hardware or a cloud VM\n",
          "- Setting up proper authentication and security measures\n",
          "- Using persistent storage for model files"
        ]
      }
    ],
    "metadata": {
      "colab": {
        "provenance": [],
        "toc_visible": true
      },
      "kernelspec": {
        "display_name": "Python 3",
        "name": "python3"
      },
      "language_info": {
        "name": "python"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 0
  }
]