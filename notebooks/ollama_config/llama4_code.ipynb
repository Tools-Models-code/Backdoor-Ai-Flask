{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollama-header"
      },
      "source": [
        "# Backdoor AI - Llama4 Code on Google Colab\n",
        "\n",
        "This notebook helps you run **Llama4 Code** on Google Colab to use with your Backdoor AI application. Llama4 Code is specifically optimized for programming tasks, code completion, and technical reasoning.\n",
        "\n",
        "## How it works\n",
        "\n",
        "1. This notebook will first optimize your Colab environment for large models\n",
        "2. We'll install Ollama on this Colab instance\n",
        "3. You'll download the Llama4 Code model, which is optimized for programming\n",
        "4. We'll set up Cloudflared to create a secure tunnel to your Ollama instance\n",
        "5. You'll get a URL to use in your Backdoor AI settings\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memory-optimization"
      },
      "source": [
        "## 1. Memory Optimization for Large Models\n",
        "\n",
        "First, let's clear up disk space and optimize memory to ensure we have enough resources for the Llama4 Code model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-optimization-code"
      },
      "outputs": [],
      "source": [
        "# Memory optimization functions\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import gc\n",
        "import time\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Install required packages first\n",
        "!pip install -q psutil\n",
        "import psutil\n",
        "\n",
        "def clear_disk_space():\n",
        "    \"\"\"Clean up disk space by removing unnecessary files.\"\"\"\n",
        "    print(\"\ud83e\uddf9 Cleaning up disk space...\")\n",
        "    \n",
        "    # Clean apt cache\n",
        "    subprocess.run(\"apt-get clean\", shell=True)\n",
        "    \n",
        "    # Remove unnecessary packages\n",
        "    subprocess.run(\"apt-get -y autoremove\", shell=True)\n",
        "    \n",
        "    # Clean pip cache\n",
        "    subprocess.run(\"rm -rf ~/.cache/pip\", shell=True)\n",
        "    \n",
        "    # Remove temporary files\n",
        "    temp_dirs = ['/tmp', '/var/tmp']\n",
        "    for temp_dir in temp_dirs:\n",
        "        if os.path.exists(temp_dir):\n",
        "            try:\n",
        "                for item in os.listdir(temp_dir):\n",
        "                    item_path = os.path.join(temp_dir, item)\n",
        "                    # Skip our ollama directories\n",
        "                    if item.startswith('ollama') or item.startswith('backdoor'):\n",
        "                        continue\n",
        "                    \n",
        "                    try:\n",
        "                        if os.path.isdir(item_path):\n",
        "                            shutil.rmtree(item_path)\n",
        "                        else:\n",
        "                            os.remove(item_path)\n",
        "                    except Exception as e:\n",
        "                        pass  # Skip files that can't be removed\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not clean {temp_dir}: {e}\")\n",
        "    \n",
        "    # Remove unused Docker images/containers if Docker is installed\n",
        "    try:\n",
        "        subprocess.run(\"docker system prune -af\", shell=True, stderr=subprocess.DEVNULL)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"\u2705 Disk cleanup complete!\")\n",
        "    \n",
        "    # Show disk space\n",
        "    show_disk_usage()\n",
        "\n",
        "def show_disk_usage():\n",
        "    \"\"\"Show current disk usage.\"\"\"\n",
        "    try:\n",
        "        df_output = subprocess.check_output(\"df -h /\", shell=True, text=True)\n",
        "        print(\"\\n\ud83d\udcca Disk Space Available:\")\n",
        "        for line in df_output.split('\\n'):\n",
        "            print(line)\n",
        "    except:\n",
        "        print(\"Could not retrieve disk usage information\")\n",
        "\n",
        "def show_memory_usage():\n",
        "    \"\"\"Show current memory usage.\"\"\"\n",
        "    try:\n",
        "        memory = psutil.virtual_memory()\n",
        "        total_gb = memory.total / (1024 ** 3)\n",
        "        available_gb = memory.available / (1024 ** 3)\n",
        "        used_gb = memory.used / (1024 ** 3)\n",
        "        percent = memory.percent\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcca Memory Usage:\")\n",
        "        print(f\"Total Memory: {total_gb:.2f} GB\")\n",
        "        print(f\"Available: {available_gb:.2f} GB\")\n",
        "        print(f\"Used: {used_gb:.2f} GB ({percent}%)\")\n",
        "    except:\n",
        "        print(\"Could not retrieve memory usage information\")\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear Python memory.\"\"\"\n",
        "    gc.collect()\n",
        "    torch_available = False\n",
        "    \n",
        "    try:\n",
        "        import torch\n",
        "        torch_available = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    if torch_available:\n",
        "        try:\n",
        "            import torch\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"\u2705 PyTorch CUDA cache cleared\")\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    print(\"\u2705 Python memory cleared\")\n",
        "\n",
        "def clean_model_files(keep_models=None):\n",
        "    \"\"\"Clean up model files to free space, optionally keeping specified models.\"\"\"\n",
        "    if keep_models is None:\n",
        "        keep_models = []\n",
        "    \n",
        "    print(f\"\ud83e\uddf9 Cleaning model files (keeping: {', '.join(keep_models) if keep_models else 'none'})...\")\n",
        "    \n",
        "    # Clean Ollama model files (except the ones specified to keep)\n",
        "    ollama_dirs = ['/root/.ollama', '/tmp/ollama']\n",
        "    \n",
        "    for ollama_dir in ollama_dirs:\n",
        "        if os.path.exists(ollama_dir):\n",
        "            models_path = os.path.join(ollama_dir, 'models')\n",
        "            if os.path.exists(models_path):\n",
        "                for model_file in os.listdir(models_path):\n",
        "                    should_keep = False\n",
        "                    for keep_model in keep_models:\n",
        "                        if keep_model in model_file:\n",
        "                            should_keep = True\n",
        "                            break\n",
        "                    \n",
        "                    if not should_keep:\n",
        "                        try:\n",
        "                            model_path = os.path.join(models_path, model_file)\n",
        "                            if os.path.isdir(model_path):\n",
        "                                shutil.rmtree(model_path)\n",
        "                            else:\n",
        "                                os.remove(model_path)\n",
        "                            print(f\"  - Removed: {model_file}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"  - Could not remove {model_file}: {e}\")\n",
        "    \n",
        "    print(\"\u2705 Model cleanup complete!\")\n",
        "    show_disk_usage()\n",
        "\n",
        "def monitor_download_progress(model_name):\n",
        "    \"\"\"Monitor the download progress of a model.\"\"\"\n",
        "    last_size = 0\n",
        "    download_dir = '/root/.ollama/models'\n",
        "    \n",
        "    print(f\"\ud83d\udd04 Monitoring download progress for {model_name}\")\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            if not os.path.exists(download_dir):\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "                \n",
        "            total_size = 0\n",
        "            for root, dirs, files in os.walk(download_dir):\n",
        "                for file in files:\n",
        "                    if model_name.lower() in file.lower():\n",
        "                        try:\n",
        "                            file_path = os.path.join(root, file)\n",
        "                            total_size += os.path.getsize(file_path)\n",
        "                        except:\n",
        "                            pass\n",
        "            \n",
        "            if total_size > last_size:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Downloading {model_name}...\")\n",
        "                print(f\"Downloaded: {total_size / (1024**3):.2f} GB\")\n",
        "                last_size = total_size\n",
        "            \n",
        "            time.sleep(2)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Download monitoring stopped\")\n",
        "\n",
        "# Run optimization process\n",
        "print(\"\ud83d\ude80 Optimizing environment for Llama4 Code model...\")\n",
        "clear_disk_space()\n",
        "clear_memory()\n",
        "\n",
        "# Set environment variables for improved performance\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "\n",
        "# Show current resource usage\n",
        "show_memory_usage()\n",
        "show_disk_usage()\n",
        "\n",
        "print(\"\\n\u2705 Optimization complete! Ready to continue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "requirements-section"
      },
      "source": [
        "## 2. Set up environment\n",
        "\n",
        "Now let's install the required packages. We'll need Ollama and Cloudflared for tunneling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-requirements"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "# Install GPU detection tools first\n",
        "!apt-get update && apt-get install -y lspci lshw pciutils\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install cloudflared for tunneling\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install -q requests pyngrok httpx ipywidgets\n",
        "\n",
        "# Set up directories\n",
        "!mkdir -p /tmp/ollama/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start-ollama"
      },
      "source": [
        "## 2. Start Ollama server\n",
        "\n",
        "Now we'll start the Ollama server in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start-ollama-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Start Ollama server in background\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Wait for Ollama to start\n",
        "print(\"Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if Ollama is running\n",
        "try:\n",
        "    response = requests.get(\"http://localhost:11434/api/version\")\n",
        "    if response.status_code == 200:\n",
        "        print(f\"\u2705 Ollama is running! Version: {response.json().get('version')}\")\n",
        "    else:\n",
        "        print(f\"\u274c Ollama returned unexpected status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Failed to connect to Ollama: {e}\")\n",
        "    print(\"Trying to start again...\")\n",
        "    # Kill the previous process if it exists\n",
        "    if ollama_process:\n",
        "        ollama_process.terminate()\n",
        "        time.sleep(2)\n",
        "    # Try starting again\n",
        "    !ollama serve &\n",
        "    time.sleep(5)\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/version\")\n",
        "        if response.status_code == 200:\n",
        "            print(f\"\u2705 Second attempt succeeded! Ollama is running. Version: {response.json().get('version')}\")\n",
        "    except:\n",
        "        print(\"\u274c Failed to start Ollama after multiple attempts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-section"
      },
      "source": [
        "## 3. Download Llama4 Code Model\n",
        "\n",
        "Now, let's download the Llama4 Code model, which is specially trained for programming tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-llama4-code"
      },
      "outputs": [],
      "source": [
        "# Download Llama4 Code\n",
        "print(\"\ud83d\ude80 Downloading Llama4 Code model...\")\n",
        "print(\"This may take a while depending on your internet connection...\")\n",
        "print(\"You'll see progress below. Please don't interrupt the process.\")\n",
        "\n",
        "# Run the download command\n",
        "!ollama pull llama4-code:latest\n",
        "\n",
        "# Verify the model is available\n",
        "print(\"\\n\ud83d\udccb Available models:\")\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-model"
      },
      "source": [
        "## 4. Test the Llama4 Code model\n",
        "\n",
        "Let's make sure the model works by asking it to solve a coding problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-llama4-code"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Define a coding task\n",
        "coding_prompt = \"\"\"\n",
        "Write a Python function that finds all prime numbers up to a given limit using the Sieve of Eratosthenes algorithm.\n",
        "Include a clear explanation of how the algorithm works.\n",
        "\"\"\"\n",
        "\n",
        "# Set up the API call\n",
        "url = \"http://localhost:11434/api/chat\"\n",
        "payload = {\n",
        "    \"model\": \"llama4-code:latest\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": coding_prompt}\n",
        "    ],\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "# Make the API call\n",
        "try:\n",
        "    print(\"Testing Llama4 Code with a programming task...\\n\")\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        content = result.get(\"message\", {}).get(\"content\", \"No content returned\")\n",
        "        print(\"\u2705 Model response:\\n\")\n",
        "        print(content)\n",
        "    else:\n",
        "        print(f\"\u274c Error: Server returned status {response.status_code}\")\n",
        "        print(response.text)\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error testing model: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-tunnel"
      },
      "source": [
        "## 5. Set up a tunnel to access your Ollama instance\n",
        "\n",
        "Now we'll set up a Cloudflare tunnel so your Backdoor AI application can access this Ollama instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunnel-code"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Function to run cloudflared tunnel in a separate thread\n",
        "def run_tunnel():\n",
        "    global tunnel_process, tunnel_url\n",
        "    tunnel_process = subprocess.Popen(\n",
        "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:11434\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "    \n",
        "    # Extract tunnel URL\n",
        "    tunnel_url = None\n",
        "    url_pattern = re.compile(r'https://[\\w.-]+\\.trycloudflare\\.com')\n",
        "    \n",
        "    while True:\n",
        "        line = tunnel_process.stdout.readline()\n",
        "        if not line and tunnel_process.poll() is not None:\n",
        "            break\n",
        "        \n",
        "        print(line.strip())\n",
        "        match = url_pattern.search(line)\n",
        "        if match and not tunnel_url:\n",
        "            tunnel_url = match.group(0)\n",
        "            tunnel_info.value = f\"<div style='padding: 10px; background-color: #e6ffe6; border-radius: 5px;'><b>\u2705 Your Ollama API is accessible at:</b><br><code>{tunnel_url}</code><br><br>Use this URL in your Backdoor AI settings as the Ollama API Base URL.<br><br><b>Important:</b> In your Backdoor settings, make sure to select:<br>- Provider: Ollama<br>- Model: llama4-code:latest<br><br>Keep this notebook running while you're using Ollama with your app!</div>\"\n",
        "\n",
        "# Initialize global variables\n",
        "tunnel_process = None\n",
        "tunnel_url = None\n",
        "\n",
        "# Create tunnel info widget\n",
        "tunnel_info = widgets.HTML(\"<div style='padding: 10px; background-color: #fff3e6; border-radius: 5px;'>\u23f3 Creating secure tunnel to Ollama... (this may take a moment)</div>\")\n",
        "display(tunnel_info)\n",
        "\n",
        "# Start tunnel\n",
        "thread = threading.Thread(target=run_tunnel)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Wait for tunnel URL\n",
        "attempts = 0\n",
        "while attempts < 30 and not tunnel_url:\n",
        "    time.sleep(1)\n",
        "    attempts += 1\n",
        "    \n",
        "if not tunnel_url:\n",
        "    tunnel_info.value = \"<div style='padding: 10px; background-color: #ffe6e6; border-radius: 5px;'>\u274c Failed to create tunnel. Check the output below for details.</div>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "backdoor-instructions"
      },
      "source": [
        "## 6. Connect Backdoor AI to your Llama4 Code instance\n",
        "\n",
        "Once you have your tunnel URL, follow these steps to connect Backdoor AI to your Llama4 Code instance:\n",
        "\n",
        "1. Go to your Backdoor AI settings page\n",
        "2. Select the \"Ollama\" tab\n",
        "3. Select \"Use Google Colab\" as your setup method\n",
        "4. In the \"Ollama API URL\" field, enter the tunnel URL from above\n",
        "5. In the \"Ollama Model\" dropdown, select \"llama4-code:latest\"\n",
        "6. Click \"Save Settings\"\n",
        "\n",
        "### Benefits of Llama4 Code for Backdoor AI\n",
        "\n",
        "Llama4 Code is optimized for programming tasks and provides enhanced capabilities for:\n",
        "- Code completion and generation\n",
        "- Debugging assistance\n",
        "- Technical explanation\n",
        "- API and framework understanding\n",
        "- Software design and architecture\n",
        "\n",
        "**Important Notes:**\n",
        "- Keep this notebook running as long as you're using Ollama with your app\n",
        "- The tunnel URL will change each time you restart this notebook\n",
        "- Google Colab sessions have limited runtime (a few hours for free tier)\n",
        "- Your model downloads will be lost when the Colab session ends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keep-alive"
      },
      "source": [
        "## 7. Keep the Colab session alive\n",
        "\n",
        "Run the cell below to prevent Colab from disconnecting due to inactivity. This will create a small animation that keeps the session active."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keep-alive-code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import IPython.display\n",
        "from IPython.display import HTML, display\n",
        "import threading\n",
        "\n",
        "def keep_alive():\n",
        "    while True:\n",
        "        time.sleep(60)  # Update every minute\n",
        "        IPython.display.clear_output(wait=True)\n",
        "        display(HTML(f'''\n",
        "        <div style=\"padding: 10px; background-color: #f0f9ff; border-radius: 5px; width: 100%;\">\n",
        "            <h3>\ud83d\udce1 Ollama Server Status</h3>\n",
        "            <p><b>Tunnel URL:</b> {tunnel_url or \"Not available\"}</p>\n",
        "            <p><b>Model:</b> llama4-code:latest</p>\n",
        "            <p><b>Session active for:</b> {int(time.time() - start_time)} seconds</p>\n",
        "            <p><b>Status:</b> Running</p>\n",
        "            <div style=\"margin-top:10px; text-align:center;\">\n",
        "                <div class=\"spinner\" style=\"display: inline-block; width: 20px; height: 20px; border: 3px solid rgba(0,0,0,.3); border-radius: 50%; border-top-color: #3498db; animation: spin 1s ease-in-out infinite;\"></div>\n",
        "                <style>\n",
        "                    @keyframes spin { to { transform: rotate(360deg); } }\n",
        "                </style>\n",
        "            </div>\n",
        "        </div>\n",
        "        '''))\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Start the keep-alive thread\n",
        "keep_alive_thread = threading.Thread(target=keep_alive)\n",
        "keep_alive_thread.daemon = True\n",
        "keep_alive_thread.start()\n",
        "\n",
        "# Initial display\n",
        "display(HTML(f'''\n",
        "<div style=\"padding: 10px; background-color: #f0f9ff; border-radius: 5px; width: 100%;\">\n",
        "    <h3>\ud83d\udce1 Ollama Server Status</h3>\n",
        "    <p><b>Tunnel URL:</b> {tunnel_url or \"Not available\"}</p>\n",
        "    <p><b>Model:</b> llama4-code:latest</p>\n",
        "    <p><b>Session active for:</b> 0 seconds</p>\n",
        "    <p><b>Status:</b> Running</p>\n",
        "    <div style=\"margin-top:10px; text-align:center;\">\n",
        "        <div class=\"spinner\" style=\"display: inline-block; width: 20px; height: 20px; border: 3px solid rgba(0,0,0,.3); border-radius: 50%; border-top-color: #3498db; animation: spin 1s ease-in-out infinite;\"></div>\n",
        "        <style>\n",
        "            @keyframes spin { to { transform: rotate(360deg); } }\n",
        "        </style>\n",
        "    </div>\n",
        "</div>\n",
        "'''))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}